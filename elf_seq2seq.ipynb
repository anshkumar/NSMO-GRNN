{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elf_seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshkumar/elf/blob/master/elf_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "74JLaETpMp5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08116a9a-92d7-4b60-c479-142084498dd0"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L4sTGwL8zyEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d2f5e38b-43f1-482d-b4cc-eab9a3a1ca68"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-kmVZqCe6faX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp \"./drive/My Drive/data.zip\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bxu8d4Pe8xa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3612
        },
        "outputId": "317737be-3628-4246-a977-35a9472416a9"
      },
      "cell_type": "code",
      "source": [
        "!unzip ./data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./data.zip\n",
            "   creating: data/\n",
            "   creating: data/VIC/\n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201501_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201711_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201710_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201612_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201706_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201707_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201604_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201605_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201510_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201511_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201701_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201609_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201608_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201603_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201602_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201507_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201506_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201607_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201606_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201705_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201704_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201509_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201508_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201610_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201611_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201503_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201502_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201712_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201504_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201505_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201601_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201708_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201709_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201512_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201702_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201703_VIC1.csv  \n",
            "   creating: data/vars/\n",
            "  inflating: data/vars/.DS_Store     \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/data/\n",
            "   creating: __MACOSX/data/vars/\n",
            "  inflating: __MACOSX/data/vars/._.DS_Store  \n",
            "  inflating: data/vars/feeder_meta.pkl  \n",
            "  inflating: data/vars/feeder.cpt.index  \n",
            "  inflating: data/vars/feeder.cpt.data-00000-of-00001  \n",
            "  inflating: data/.DS_Store          \n",
            "  inflating: data/all.pkl            \n",
            "   creating: data/SA/\n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201705_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201502_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201512_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201606_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201503_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201607_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201704_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201605_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201501_SA1.csv  \n",
            "   creating: __MACOSX/data/SA/\n",
            "  inflating: __MACOSX/data/SA/._PRICE_AND_DEMAND_201501_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201511_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201706_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201508_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201707_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201509_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201604_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201510_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201601_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201611_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201505_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201702_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201712_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201608_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201703_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201609_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201610_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201504_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201701_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201711_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201708_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201506_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201602_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201612_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201709_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201507_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201603_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201710_SA1.csv  \n",
            "   creating: data/QLD/\n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201504_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201505_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201708_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201709_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201601_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201512_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201702_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201703_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201607_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201606_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201705_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201704_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201509_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201508_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201610_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201611_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201503_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201502_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201712_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201510_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201511_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201609_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201608_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201701_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201603_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201602_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201507_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201506_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201501_QLD1.csv  \n",
            "   creating: __MACOSX/data/QLD/\n",
            "  inflating: __MACOSX/data/QLD/._PRICE_AND_DEMAND_201501_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201711_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201710_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201612_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201706_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201707_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201604_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201605_QLD1.csv  \n",
            "   creating: data/NSW/\n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201506_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201507_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201608_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201609_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201701_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201511_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201510_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201602_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201603_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201707_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201706_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201605_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201604_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201710_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201711_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201501_NSW1.csv  \n",
            "   creating: __MACOSX/data/NSW/\n",
            "  inflating: __MACOSX/data/NSW/._PRICE_AND_DEMAND_201501_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201612_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201709_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201708_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201601_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201703_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201702_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201512_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201505_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201504_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201611_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201610_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201508_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201509_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201712_NSW1.csv  \n",
            "  inflating: __MACOSX/data/NSW/._PRICE_AND_DEMAND_201712_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201502_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201503_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201606_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201607_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201704_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201705_NSW1.csv  \n",
            "   creating: data/TAS/\n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201602_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201603_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201701_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201608_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201609_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201511_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201510_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201506_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201507_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201612_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201710_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201711_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201501_TAS1.csv  \n",
            "   creating: __MACOSX/data/TAS/\n",
            "  inflating: __MACOSX/data/TAS/._PRICE_AND_DEMAND_201501_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201605_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201604_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201707_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201706_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201505_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201504_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201703_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201702_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201512_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201601_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201709_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201708_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201704_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201705_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201606_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201607_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201712_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201502_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201503_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201611_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201610_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201508_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201509_TAS1.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AxMVqA62NvXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e43ecb03-40d3-416c-f4d9-a087fca516da"
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.7)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 137.7 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MdLtdR8KMHMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 19:05:59 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from collections import UserList, UserDict\n",
        "from typing import Union, Iterable, Tuple, Dict, Any\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os.path\n",
        "\n",
        "def _meta_file(path):\n",
        "    return os.path.join(path, 'feeder_meta.pkl')\n",
        "\n",
        "\n",
        "class VarFeeder:\n",
        "    \"\"\"\n",
        "    Builds temporary TF graph, injects variables into, and saves variables to TF checkpoint.\n",
        "    In a train time, variables can be built by build_vars() and content restored by FeederVars.restore()\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str,\n",
        "                 tensor_vars: Dict[str, Union[pd.DataFrame, pd.Series, np.ndarray]] = None,\n",
        "                 plain_vars: Dict[str, Any] = None):\n",
        "        \"\"\"\n",
        "        :param path: dir to store data\n",
        "        :param tensor_vars: Variables to save as Tensors (pandas DataFrames/Series or numpy arrays)\n",
        "        :param plain_vars: Variables to save as Python objects\n",
        "        \"\"\"\n",
        "        tensor_vars = tensor_vars or dict()\n",
        "\n",
        "        def get_values(v):\n",
        "            v = v.values if hasattr(v, 'values') else v\n",
        "            if not isinstance(v, np.ndarray):\n",
        "                v = np.array(v)\n",
        "            if v.dtype == np.float64:\n",
        "                v = v.astype(np.float32)\n",
        "            return v\n",
        "\n",
        "        values = [get_values(var) for var in tensor_vars.values()]\n",
        "\n",
        "        self.shapes = [var.shape for var in values]\n",
        "        self.dtypes = [v.dtype for v in values]\n",
        "        self.names = list(tensor_vars.keys())\n",
        "        self.path = path\n",
        "        self.plain_vars = plain_vars\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "        with open(_meta_file(path), mode='wb') as file:\n",
        "            pickle.dump(self, file)\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            tensor_vars = self._build_vars()\n",
        "            placeholders = [tf.placeholder(tf.as_dtype(dtype), shape=shape) for dtype, shape in\n",
        "                            zip(self.dtypes, self.shapes)]\n",
        "            assigners = [tensor_var.assign(placeholder) for tensor_var, placeholder in\n",
        "                         zip(tensor_vars, placeholders)]\n",
        "            feed = {ph: v for ph, v in zip(placeholders, values)}\n",
        "            saver = tf.train.Saver(self._var_dict(tensor_vars), max_to_keep=1)\n",
        "            init = tf.global_variables_initializer()\n",
        "\n",
        "            with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\n",
        "                sess.run(init)\n",
        "                sess.run(assigners, feed_dict=feed)\n",
        "                save_path = os.path.join(path, 'feeder.cpt')\n",
        "                saver.save(sess, save_path, write_meta_graph=False, write_state=False)\n",
        "\n",
        "    def _var_dict(self, variables):\n",
        "        return {name: var for name, var in zip(self.names, variables)}\n",
        "\n",
        "    def _build_vars(self):\n",
        "        def make_tensor(shape, dtype, name):\n",
        "            tf_type = tf.as_dtype(dtype)\n",
        "            if tf_type == tf.string:\n",
        "                empty = ''\n",
        "            elif tf_type == tf.bool:\n",
        "                empty = False\n",
        "            else:\n",
        "                empty = 0\n",
        "            init = tf.constant(empty, shape=shape, dtype=tf_type)\n",
        "            return tf.get_local_variable(name=name, initializer=init, dtype=tf_type)\n",
        "\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            with tf.name_scope('feeder_vars'):\n",
        "                return [make_tensor(shape, dtype, name) for shape, dtype, name in\n",
        "                        zip(self.shapes, self.dtypes, self.names)]\n",
        "\n",
        "    def create_vars(self):\n",
        "        \"\"\"\n",
        "        Builds variable list to use in current graph. Should be called during graph building stage\n",
        "        :return: variable list with additional restore and create_saver methods\n",
        "        \"\"\"\n",
        "        return FeederVars(self._var_dict(self._build_vars()), self.plain_vars, self.path)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_vars(path):\n",
        "        with open(_meta_file(path), mode='rb') as file:\n",
        "            feeder = pickle.load(file)\n",
        "        assert feeder.path == path\n",
        "        return feeder.create_vars()\n",
        "\n",
        "\n",
        "class FeederVars(UserDict):\n",
        "    def __init__(self, tensors: dict, plain_vars: dict, path):\n",
        "        variables = dict(tensors)\n",
        "        if plain_vars:\n",
        "            variables.update(plain_vars)\n",
        "        super().__init__(variables)\n",
        "        self.path = path\n",
        "        self.saver = tf.train.Saver(tensors, name='varfeeder_saver')\n",
        "        for var in variables:\n",
        "            if var not in self.__dict__:\n",
        "                self.__dict__[var] = variables[var]\n",
        "\n",
        "    def restore(self, session):\n",
        "        \"\"\"\n",
        "        Restores variable content\n",
        "        :param session: current session\n",
        "        :return: variable list\n",
        "        \"\"\"\n",
        "        self.saver.restore(session, os.path.join(self.path, 'feeder.cpt'))\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Vw4L1iCMWAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ddcd73c3-2239-4390-9a4a-73eb8472e83b"
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Oct  4 12:02:32 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def read_all() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads source data for training/prediction\n",
        "    \"\"\"\n",
        "    # Path to cached data\n",
        "    path = os.path.join('data', 'all.pkl')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_pickle(path)\n",
        "        return df\n",
        "    else:\n",
        "        state = {0: 'NSW', 1: 'QLD', 2: 'SA', 3: 'TAS', 4: 'VIC'}\n",
        "        year = {0: '2015', 1: '2016', 2: '2017'}\n",
        "        \n",
        "        df_nsw = pd.DataFrame()\n",
        "        df_qld = pd.DataFrame()\n",
        "        df_sa = pd.DataFrame()\n",
        "        df_tas = pd.DataFrame()\n",
        "        df_vic = pd.DataFrame()\n",
        "        \n",
        "        df_all = pd.DataFrame()\n",
        "        \n",
        "        df = {'NSW': df_nsw, 'QLD': df_qld, 'SA': df_sa, 'TAS': df_tas, 'VIC': df_vic}\n",
        "        \n",
        "        for st in state.values():\n",
        "            for ye in year.values():\n",
        "                for mn in range(1,13):\n",
        "                    if mn < 10:            \n",
        "                        dataset = pd.read_csv('./data/' + st + '/PRICE_AND_DEMAND_' + ye + '0' + str(mn) +'_' + st + '1.csv')\n",
        "                    else:\n",
        "                        dataset = pd.read_csv('./data/' + st + '/PRICE_AND_DEMAND_' + ye + str(mn) +'_' + st + '1.csv')\n",
        "                    df[st] = df[st].append(dataset.iloc[:,:3])\n",
        "            df_all = df_all.append(df[st].iloc[:,:])\n",
        "        df_all = df_all.set_index(['REGION', 'SETTLEMENTDATE'])\n",
        "        df_all.to_pickle(path)\n",
        "        return df_all\n",
        "    \n",
        "def read_x(state, start, end) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Gets source data from start to end date. Any date can be None\n",
        "    \"\"\"\n",
        "    def read_state(state):\n",
        "        df = read_all()\n",
        "        return df.loc[state]\n",
        "    df = read_state(state)\n",
        "    if start and end:\n",
        "        return df.loc[start:end]\n",
        "    elif end:\n",
        "        return df.loc[:end]\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def single_autocorr(series, lag):\n",
        "    \"\"\"\n",
        "    Autocorrelation for single data series\n",
        "    :param series: traffic series\n",
        "    :param lag: lag, days\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    s1 = series[lag:]\n",
        "    s2 = series[:-lag]\n",
        "    ms1 = np.mean(s1)\n",
        "    ms2 = np.mean(s2)\n",
        "    ds1 = s1 - ms1\n",
        "    ds2 = s2 - ms2\n",
        "    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n",
        "    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n",
        "\n",
        "def batch_autocorr(series, lag, starts, ends, backoffset=0):\n",
        "    \"\"\"\n",
        "    Calculate autocorrelation for batch (many time series at once)\n",
        "    :param data: Time series\n",
        "    :param lag: Autocorrelation lag\n",
        "    :param starts: Start index for series\n",
        "    :param ends: End index for series\n",
        "    :param backoffset: Offset from the series end, days.\n",
        "    :return: autocorrelation, shape [n_series].\n",
        "    \"\"\"\n",
        "    n_series = series.shape[0]\n",
        "    max_end = n_series - backoffset\n",
        "    \n",
        "    end = min(ends, max_end)\n",
        "    series = series[starts:end]\n",
        "    c_365 = single_autocorr(series, lag)\n",
        "    c_364 = single_autocorr(series, lag-1)\n",
        "    c_366 = single_autocorr(series, lag+1)\n",
        "    # Average value between exact lag and two nearest neighborhs for smoothness\n",
        "    corr = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n",
        "    return corr #, support\n",
        "\n",
        "def prepare_data(start, end, state) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(series)\n",
        "    :param start: start date of effective time interval, can be None to start from beginning\n",
        "    :param end: end date of effective time interval, can be None to return all data\n",
        "    :param state: state of series\n",
        "    :return: log1p(series)\n",
        "    \"\"\"\n",
        "    df = read_x(state, start, end)\n",
        "    return np.log1p(df.fillna(0))\n",
        "\n",
        "def normalize(values: np.ndarray):\n",
        "    return (values - values.mean()) / np.std(values)\n",
        "\n",
        "def lag_indexes(begin, end) -> List[pd.Series]:\n",
        "    \"\"\"\n",
        "    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n",
        "    :param begin: start of date range\n",
        "    :param end: end of date range\n",
        "    :return: List of 4 Series, one for each lag. For each Series, index is date in range(begin, end), value is an index\n",
        "     of target (lagged) date in a same Series. If target date is out of (begin,end) range, index is -1\n",
        "    \"\"\"\n",
        "    dr = pd.date_range(begin, end, freq='0.5H')\n",
        "    # key is date, value is day index\n",
        "    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n",
        "\n",
        "    def lag(offset):\n",
        "        dates = dr - offset\n",
        "        return pd.Series(data=base_index.loc[dates].fillna(-1).astype(np.int32).values, index=dr)\n",
        "\n",
        "    return [lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]\n",
        "\n",
        "def run():\n",
        "    \n",
        "    data_dir = \"data/vars\"\n",
        "    state = \"NSW1\"\n",
        "    add_end = 0\n",
        "    start = \"2015/01/01 00:30:00\"\n",
        "    end = \"2018/01/01 00:00:00\"\n",
        "    corr_backoffset = 0\n",
        "    \n",
        "    # Get the data\n",
        "    df = prepare_data(start, end, state)\n",
        "    \n",
        "    # Our working date range\n",
        "    data_start, data_end = pd.to_datetime(df.first_valid_index()), pd.to_datetime(df.last_valid_index())\n",
        "    \n",
        "    # We have to project some date-dependent features (day of week, etc) to the future dates for prediction\n",
        "    features_end = data_end + pd.Timedelta(add_end/2, unit='h')\n",
        "    print(f\"start: {data_start}, end:{data_end}, features_end:{features_end}\")\n",
        "    \n",
        "    starts = 0\n",
        "    ends = df.shape[0]\n",
        "    \n",
        "    # Yearly(annual) autocorrelation\n",
        "    year_autocorr = batch_autocorr(df.values, 365, starts, ends, corr_backoffset)\n",
        "    \n",
        "    # Quarterly autocorrelation\n",
        "    quarter_autocorr = batch_autocorr(df.values, int(round(365.25/4)), starts, ends, corr_backoffset)\n",
        "    \n",
        "    # Make time-dependent features\n",
        "    features = pd.date_range(data_start, features_end, freq='0.5H')\n",
        "    #dow = normalize(features_days.dayofweek.values)\n",
        "    week_period = 7 / (2 * np.pi)\n",
        "    dow_norm = features.dayofweek.values / week_period\n",
        "    dow = np.stack([np.cos(dow_norm), np.sin(dow_norm)], axis=-1)\n",
        "    \n",
        "     # Assemble indices for quarterly lagged data\n",
        "    lagged_ix = np.stack(lag_indexes(data_start, features_end), axis=-1)\n",
        "    \n",
        "    # Assemble final output\n",
        "    tensors = dict(\n",
        "        hits=df,\n",
        "        lagged_ix=lagged_ix,\n",
        "        page_ix=df.index.values,\n",
        "        dow=dow\n",
        "        )\n",
        "    plain = dict(\n",
        "        features=len(features),\n",
        "        year_autocorr=year_autocorr,\n",
        "        quarter_autocorr=quarter_autocorr,\n",
        "        data_size=df.shape[0],\n",
        "        data_start=data_start,\n",
        "        data_end=data_end,\n",
        "        features_end=features_end\n",
        "        )\n",
        "\n",
        "    # Store data to the disk\n",
        "    VarFeeder(data_dir, tensors, plain)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    python make_features.py data/vars --add_end=0 --start=\"2015/01/01 00:30:00\" --end=\"2018/01/01 00:00:00\"\n",
        "    \"\"\"\n",
        "    run()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start: 2015-01-01 00:30:00, end:2018-01-01 00:00:00, features_end:2018-01-01 00:00:00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zJABrGwVhIPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 20:09:16 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from enum import Enum\n",
        "import tensorflow as tf\n",
        "\n",
        "class ModelMode(Enum):\n",
        "  TRAIN = 0,\n",
        "  EVAL = 1,\n",
        "  PREDICT = 2\n",
        "\n",
        "class InputPipe:\n",
        "  def __init__(self, inp: VarFeeder, mode: ModelMode, n_epoch=None,\n",
        "               batch_size=32, runs_in_burst=1, verbose=True, predict_window=48, train_window=36, back_offset=0,\n",
        "               train_skip_first=0, rand_seed=None):\n",
        "      \"\"\"\n",
        "      Create data preprocessing pipeline\n",
        "      :param inp: Raw input data\n",
        "      :param features: Features tensors (subset of data in inp)\n",
        "      :param mode: Train/Predict/Eval mode selector\n",
        "      :param n_epoch: Number of epochs. Generates endless data stream if None\n",
        "      :param batch_size: (https://www.tensorflow.org/guide/datasets#batching_dataset_elements)\n",
        "      :param n_splits: number of time series split\n",
        "      :param runs_in_burst: How many batches can be consumed at short time interval (burst). Multiplicator for prefetch() (https://www.tensorflow.org/performance/datasets_performance)\n",
        "      :param verbose: Print additional information during graph construction\n",
        "      :param predict_window: Number of days to predict\n",
        "      :param train_window: Use train_window days for traning\n",
        "      :param back_offset: Don't use back_offset days at the end of timeseries\n",
        "      :param train_skip_first: Don't use train_skip_first days at the beginning of timeseries\n",
        "      :param rand_seed:\n",
        "      \"\"\"\n",
        "      self.inp = inp\n",
        "      self.batch_size = batch_size\n",
        "      self.rand_seed = rand_seed\n",
        "      self.back_offset = back_offset\n",
        "      self.start_offset = train_skip_first\n",
        "      self.train_window = train_window\n",
        "      self.predict_window = predict_window\n",
        "      self.mode = mode\n",
        "      self.verbose = verbose\n",
        "      \n",
        "      list_half_hourly_load = self.inp.hits[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      list_half_hourly_load = list_half_hourly_load / tf.linalg.norm(list_half_hourly_load)\n",
        "      matrix_hits = list_half_hourly_load[:-(list_half_hourly_load.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_hits = tf.reshape(matrix_hits, [-1, (train_window + predict_window)])\n",
        "      \n",
        "      matrix_dow = self.inp.dow[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      matrix_dow = matrix_dow[:-(matrix_dow.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_dow = tf.reshape(matrix_dow, [-1, (train_window + predict_window), 2])\n",
        "      \n",
        "      # Convert -1 to 0 for gather(), it don't accept anything exotic\n",
        "      lags_indices = tf.maximum(self.inp.lagged_ix, 0)\n",
        "      # Translate lag indexes to hit values\n",
        "      lagged_hit = tf.gather(list_half_hourly_load, lags_indices)\n",
        "      lagged_hit = lagged_hit / tf.linalg.norm(lagged_hit)\n",
        "      lagged_hit = lagged_hit[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      matrix_lagged_hit = lagged_hit[:-(lagged_hit.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_lagged_hit = tf.reshape(matrix_lagged_hit, [-1, (train_window + predict_window), 4])\n",
        "      \n",
        "      x_hits = matrix_hits[:, :train_window]\n",
        "      y_hits = matrix_hits[:, train_window:]\n",
        "      \n",
        "      x_dow = matrix_dow[:, :train_window]\n",
        "      y_dow = matrix_dow[:, train_window:]\n",
        "      \n",
        "      x_lagged_hit = matrix_lagged_hit[:, :train_window]\n",
        "      y_lagged_hit = matrix_lagged_hit[:, train_window:]\n",
        "\n",
        "      def tileFeatures(window):\n",
        "        # [n_features] => [window, n_features]\n",
        "        flat_features = tf.stack([self.inp.year_autocorr, self.inp.quarter_autocorr], axis=0 )\n",
        "        features = tf.expand_dims(flat_features, axis=0)\n",
        "        features = tf.tile(features, [window,1])\n",
        "\n",
        "        # [train_window, n_features] => [size, window, n_features]\n",
        "        features = tf.expand_dims(features, axis=0)\n",
        "        features = tf.tile(features, [x_hits.shape[0].value, 1,1])\n",
        "        return features\n",
        "      \n",
        "      x_features = tf.concat((tf.reshape(x_hits, [x_hits.shape[0],train_window,1]),\n",
        "                                   x_dow, x_lagged_hit, \n",
        "                                   tileFeatures(train_window) \n",
        "                                   ), axis = 2)\n",
        "      \n",
        "      y_features = tf.concat((y_dow, \n",
        "                              y_lagged_hit,\n",
        "                              tileFeatures(predict_window)\n",
        "                             ), axis = 2)\n",
        "      \n",
        "      y_labels = tf.reshape(y_hits, [y_hits.shape[0], predict_window, 1])\n",
        "      \n",
        "      # Assume that each row of `features` corresponds to the same row as `labels`.\n",
        "      assert x_features.shape[0] == y_labels.shape[0].value\n",
        "      \n",
        "      dataset = tf.data.Dataset.from_tensor_slices((x_features, y_features, y_labels))\n",
        "      # [Other transformations on `dataset`...]\n",
        "      #dataset = dataset.shuffle(seed=rand_seed)\n",
        "      dataset = dataset.repeat(n_epoch)  # Repeat the input indefinitely.\n",
        "      dataset = dataset.batch(batch_size)\n",
        "      dataset = dataset.prefetch(runs_in_burst * 2)\n",
        "\n",
        "      self.iterator = dataset.make_initializable_iterator()\n",
        "      self.x_feature, self.y_feature, self.y_true = self.iterator.get_next()\n",
        "      self.encoder_features_depth = self.x_feature.shape[2].value\n",
        "\n",
        "  def load_vars(self, session):\n",
        "      self.inp.restore(session)\n",
        "\n",
        "  def init_iterator(self, session):\n",
        "      session.run(self.iterator.initializer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKDG28iWVtmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 20:12:08 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.cudnn_rnn as cudnn_rnn\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "import tensorflow.contrib.layers as layers\n",
        "from tensorflow.python.util import nest\n",
        "\n",
        "GRAD_CLIP_THRESHOLD = 10\n",
        "RNN = cudnn_rnn.CudnnGRU\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + 1e-10))))\n",
        "\n",
        "def default_init(seed):\n",
        "    # xavier_initializer\n",
        "    return layers.variance_scaling_initializer(factor=1.0,\n",
        "                                               mode=\"FAN_AVG\",\n",
        "                                               uniform=True,\n",
        "                                               seed=seed)\n",
        "  \n",
        "def selu(x):\n",
        "    \"\"\"\n",
        "    SELU activation\n",
        "    https://arxiv.org/abs/1706.02515\n",
        "    :param x:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with tf.name_scope('elu') as scope:\n",
        "        alpha = 1.6732632423543772848170429916717\n",
        "        scale = 1.0507009873554804934193349852946\n",
        "        return scale * tf.where(x >= 0.0, x, alpha * tf.nn.elu(x))\n",
        "\n",
        "class Model:\n",
        "  def __init__(self, inp: InputPipe, hparams, is_train, seed, graph_prefix=None, asgd_decay=None, loss_mask=None):\n",
        "      \"\"\"\n",
        "      Encoder-decoder prediction model\n",
        "      :param inp: Input tensors\n",
        "      :param hparams:\n",
        "      :param is_train:\n",
        "      :param seed:\n",
        "      :param graph_prefix: Subgraph prefix for multi-model graph\n",
        "      :param asgd_decay: Decay for SGD averaging\n",
        "      :param loss_mask: Additional mask for losses calculation (one value for each prediction day), shape=[predict_window]\n",
        "      \"\"\"\n",
        "      self.is_train = is_train\n",
        "      self.inp = inp\n",
        "      self.hparams = hparams\n",
        "      self.seed = seed\n",
        "      self.inp = inp\n",
        "\n",
        "      encoder_output, h_state, c_state = self.encoder(inp.x_feature, inp.encoder_features_depth, is_train, hparams, seed,\n",
        "                                                      transpose_output=True)\n",
        "      # Encoder activation losses\n",
        "      enc_stab_loss = self.rnn_stability_loss(encoder_output, hparams.encoder_stability_loss / inp.train_window)\n",
        "      enc_activation_loss = self.rnn_activation_loss(encoder_output, hparams.encoder_activation_loss / inp.train_window)\n",
        "\n",
        "      # Convert state from cuDNN representation to TF RNNCell-compatible representation\n",
        "      encoder_state = self.convert_cudnn_state_v2(h_state, hparams, c_state,\n",
        "                                             dropout=hparams.gate_dropout if is_train else 1.0)\n",
        "\n",
        "      # Run decoder\n",
        "      decoder_targets, decoder_outputs = self.decoder(encoder_state,\n",
        "                                                      inp.y_feature, inp.x_feature[:, -1, 0])\n",
        "      # Decoder activation losses\n",
        "      dec_stab_loss = self.rnn_stability_loss(decoder_outputs, hparams.decoder_stability_loss / inp.predict_window)\n",
        "      dec_activation_loss = self.rnn_activation_loss(decoder_outputs, hparams.decoder_activation_loss / inp.predict_window)\n",
        "\n",
        "      # Get final denormalized predictions\n",
        "#      self.predictions = decode_predictions(decoder_targets, inp)\n",
        "      self.predictions = tf.transpose(decoder_targets, [1, 0, 2])\n",
        "\n",
        "      # Calculate losses and build training op\n",
        "      if inp.mode == ModelMode.PREDICT:\n",
        "          # Pseudo-apply ema to get variable names later in ema.variables_to_restore()\n",
        "          # This is copypaste from make_train_op()\n",
        "          if asgd_decay:\n",
        "              self.ema = tf.train.ExponentialMovingAverage(decay=asgd_decay)\n",
        "              variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "              if graph_prefix:\n",
        "                  ema_vars = [var for var in variables if var.name.startswith(graph_prefix)]\n",
        "              else:\n",
        "                  ema_vars = variables\n",
        "              self.ema.apply(ema_vars)\n",
        "      else:\n",
        "          self.mae, self.mape, smape_loss, self.smape, self.loss_item_count = self.calc_loss(self.predictions, inp.y_true,\n",
        "                                                                             additional_mask=loss_mask)\n",
        "          if is_train:\n",
        "              # Sum all losses\n",
        "              total_loss = smape_loss + enc_stab_loss + dec_stab_loss + enc_activation_loss + dec_activation_loss\n",
        "              self.train_op, self.glob_norm, self.ema = self.make_train_op(total_loss, asgd_decay, prefix=graph_prefix)\n",
        "\n",
        "\n",
        "\n",
        "              \n",
        "  # Helper functions\n",
        "  def smape_loss(self, true, predicted, weights):\n",
        "    \"\"\"\n",
        "    Differentiable SMAPE loss\n",
        "    :param true: Truth values\n",
        "    :param predicted: Predicted values\n",
        "    :param weights: Weights mask to exclude some values\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    epsilon = 0.1  # Smoothing factor, helps SMAPE to be well-behaved near zero\n",
        "    true_o = tf.expm1(true)\n",
        "    pred_o = tf.expm1(predicted)\n",
        "    summ = tf.maximum(tf.abs(true_o) + tf.abs(pred_o) + epsilon, 0.5 + epsilon)\n",
        "    smape = tf.abs(pred_o - true_o) / summ * 2.0\n",
        "    return tf.losses.compute_weighted_loss(smape, weights, loss_collection=None)\n",
        "\n",
        "  def calc_smape_rounded(self, true, predicted, weights):\n",
        "    \"\"\"\n",
        "    Calculates SMAPE on rounded submission values. Should be close to official SMAPE in competition\n",
        "    :param true:\n",
        "    :param predicted:\n",
        "    :param weights: Weights mask to exclude some values\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    n_valid = tf.reduce_sum(weights)\n",
        "    true_o = tf.round(tf.expm1(true))\n",
        "    pred_o = tf.maximum(tf.round(tf.expm1(predicted)), 0.0)\n",
        "    summ = tf.abs(true_o) + tf.abs(pred_o)\n",
        "    zeros = summ < 0.01\n",
        "    raw_smape = tf.abs(pred_o - true_o) / summ * 2.0\n",
        "    smape = tf.where(zeros, tf.zeros_like(summ, dtype=tf.float32), raw_smape)\n",
        "    return tf.reduce_sum(smape * weights) / n_valid\n",
        "\n",
        "  def decode_predictions(self, decoder_readout, inp: InputPipe):\n",
        "    \"\"\"\n",
        "    Converts normalized prediction values to log1p(pageviews), e.g. reverts normalization\n",
        "    :param decoder_readout: Decoder output, shape [n_days, batch]\n",
        "    :param inp: Input tensors\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # [n_days, batch] -> [batch, n_days]\n",
        "    batch_readout = tf.transpose(decoder_readout)\n",
        "    batch_std = tf.expand_dims(inp.norm_std, -1)\n",
        "    batch_mean = tf.expand_dims(inp.norm_mean, -1)\n",
        "    return batch_readout * batch_std + batch_mean\n",
        "\n",
        "  def calc_loss(self, predictions, true_y, additional_mask=None):\n",
        "    \"\"\"\n",
        "    Calculates losses, ignoring NaN true values (assigning zero loss to them)\n",
        "    :param predictions: Predicted values\n",
        "    :param true_y: True values\n",
        "    :param additional_mask:\n",
        "    :return: MAE loss, differentiable SMAPE loss, competition SMAPE loss\n",
        "    \"\"\"\n",
        "    # Take into account NaN's in true values\n",
        "    mask = tf.is_finite(true_y)\n",
        "    # Fill NaNs by zeros (can use any value)\n",
        "    true_y = tf.where(mask, true_y, tf.zeros_like(true_y))\n",
        "    # Assign zero weight to NaNs\n",
        "    weights = tf.to_float(mask)\n",
        "    if additional_mask is not None:\n",
        "        weights = weights * tf.expand_dims(additional_mask, axis=0)\n",
        "\n",
        "    mae_loss = tf.losses.absolute_difference(labels=true_y, predictions=predictions, weights=weights)\n",
        "    mape_loss = mape(true_y, predictions)\n",
        "    return mae_loss, mape_loss, self.smape_loss(true_y, predictions, weights), self.calc_smape_rounded(true_y, predictions,\n",
        "                                                                                  weights), tf.size(true_y)\n",
        "  \n",
        "  def make_train_op(self, loss, ema_decay=None, prefix=None):\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    glob_step = tf.train.get_global_step()\n",
        "\n",
        "    # Add regularization losses\n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    total_loss = loss + reg_losses if reg_losses else loss\n",
        "\n",
        "    # Clip gradients\n",
        "    grads_and_vars = optimizer.compute_gradients(total_loss)\n",
        "    gradients, variables = zip(*grads_and_vars)\n",
        "    clipped_gradients, glob_norm = tf.clip_by_global_norm(gradients, GRAD_CLIP_THRESHOLD)\n",
        "    sgd_op, glob_norm = optimizer.apply_gradients(zip(clipped_gradients, variables)), glob_norm\n",
        "\n",
        "    # Apply SGD averaging\n",
        "    if ema_decay:\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=ema_decay, num_updates=glob_step)\n",
        "        if prefix:\n",
        "            # Some magic to handle multiple models trained in single graph\n",
        "            ema_vars = [var for var in variables if var.name.startswith(prefix)]\n",
        "        else:\n",
        "            ema_vars = variables\n",
        "        update_ema = ema.apply(ema_vars)\n",
        "        with tf.control_dependencies([sgd_op]):\n",
        "            training_op = tf.group(update_ema)\n",
        "    else:\n",
        "        training_op = sgd_op\n",
        "        ema = None\n",
        "    return training_op, glob_norm, ema\n",
        "  \n",
        "  def rnn_stability_loss(self, rnn_output, beta):\n",
        "    \"\"\"\n",
        "    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n",
        "    https://arxiv.org/pdf/1511.08400.pdf\n",
        "    :param rnn_output: [time, batch, features]\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "    if beta == 0.0:\n",
        "        return 0.0\n",
        "    # [time, batch, features] -> [time, batch]\n",
        "    l2 = tf.sqrt(tf.reduce_sum(tf.square(rnn_output), axis=-1))\n",
        "    #  [time, batch] -> []\n",
        "    return beta * tf.reduce_mean(tf.square(l2[1:] - l2[:-1]))\n",
        "\n",
        "\n",
        "  def rnn_activation_loss(self, rnn_output, beta):\n",
        "    \"\"\"\n",
        "    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n",
        "    https://arxiv.org/pdf/1511.08400.pdf\n",
        "    :param rnn_output: [time, batch, features]\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "    if beta == 0.0:\n",
        "        return 0.0\n",
        "    return tf.nn.l2_loss(rnn_output) * beta\n",
        "  \n",
        "  def convert_cudnn_state_v2(self, h_state, hparams, seed, c_state=None, dropout=1.0):\n",
        "    \"\"\"\n",
        "    Converts RNN state tensor from cuDNN representation to TF RNNCell compatible representation.\n",
        "    :param h_state: tensor [num_layers, batch_size, depth]\n",
        "    :param c_state: LSTM additional state, should be same shape as h_state\n",
        "    :return: TF cell representation matching RNNCell.state_size structure for compatible cell\n",
        "    \"\"\"\n",
        "\n",
        "    def squeeze(seq):\n",
        "        return tuple(seq) if len(seq) > 1 else seq[0]\n",
        "\n",
        "    def wrap_dropout(structure):\n",
        "        if dropout < 1.0:\n",
        "            return nest.map_structure(lambda x: tf.nn.dropout(x, keep_prob=dropout, seed=seed), structure)\n",
        "        else:\n",
        "            return structure\n",
        "\n",
        "    # Cases:\n",
        "    # decoder_layer = encoder_layers, straight mapping\n",
        "    # encoder_layers > decoder_layers: get outputs of upper encoder layers\n",
        "    # encoder_layers < decoder_layers: feed encoder outputs to lower decoder layers, feed zeros to top layers\n",
        "    h_layers = tf.unstack(h_state)\n",
        "    if hparams.encoder_rnn_layers >= hparams.decoder_rnn_layers:\n",
        "        return squeeze(wrap_dropout(h_layers[hparams.encoder_rnn_layers - hparams.decoder_rnn_layers:]))\n",
        "    else:\n",
        "        lower_inputs = wrap_dropout(h_layers)\n",
        "        upper_inputs = [tf.zeros_like(h_layers[0]) for _ in\n",
        "                        range(hparams.decoder_rnn_layers - hparams.encoder_rnn_layers)]\n",
        "        return squeeze(lower_inputs + upper_inputs)\n",
        "\n",
        "\n",
        "  def encoder(self, time_inputs, encoder_features_depth, is_train, hparams, seed, transpose_output=True):\n",
        "    \"\"\"\n",
        "    Builds encoder, using CUDA RNN\n",
        "    :param time_inputs: Input tensor, shape [batch, time, features]\n",
        "    :param encoder_features_depth: Static size for features dimension\n",
        "    :param is_train:\n",
        "    :param hparams:\n",
        "    :param seed:\n",
        "    :param transpose_output: Transform RNN output to batch-first shape\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    def build_rnn():\n",
        "        return RNN(num_layers=hparams.encoder_rnn_layers, num_units=hparams.rnn_depth,\n",
        "                   direction='unidirectional',\n",
        "                   dropout=hparams.encoder_dropout if is_train else 0, seed=seed)\n",
        "\n",
        "    cuda_model = build_rnn()\n",
        "\n",
        "    # [batch, time, features] -> [time, batch, features]\n",
        "    time_first = tf.transpose(time_inputs, [1, 0, 2])\n",
        "    rnn_time_input = time_first\n",
        "    if RNN == tf.contrib.cudnn_rnn.CudnnLSTM:\n",
        "        rnn_out, (rnn_state, c_state) = cuda_model(inputs=rnn_time_input)\n",
        "    else:\n",
        "        rnn_out, (rnn_state,) = cuda_model(inputs=rnn_time_input)\n",
        "        c_state = None\n",
        "    if transpose_output:\n",
        "        rnn_out = tf.transpose(rnn_out, [1, 0, 2])\n",
        "    return rnn_out, rnn_state, c_state\n",
        "\n",
        "  def decoder(self, encoder_state, prediction_inputs, previous_y):\n",
        "      \"\"\"\n",
        "      :param encoder_state: shape [batch_size, encoder_rnn_depth]\n",
        "      :param prediction_inputs: features for prediction days, tensor[batch_size, time, input_depth]\n",
        "      :param previous_y: Last day pageviews, shape [batch_size]\n",
        "      :return: decoder rnn output\n",
        "      \"\"\"\n",
        "      hparams = self.hparams\n",
        "\n",
        "      def build_cell(idx):\n",
        "          with tf.variable_scope('decoder_cell'):\n",
        "              cell = rnn.GRUBlockCell(self.hparams.rnn_depth)\n",
        "              has_dropout = hparams.decoder_input_dropout[idx] < 1 \\\n",
        "                            or hparams.decoder_state_dropout[idx] < 1 or hparams.decoder_output_dropout[idx] < 1\n",
        "\n",
        "              if self.is_train and has_dropout:\n",
        "                  input_size = prediction_inputs.shape[-1].value + 1 if idx == 0 else self.hparams.rnn_depth\n",
        "                  cell = rnn.DropoutWrapper(cell, dtype=tf.float32, input_size=input_size,\n",
        "                                            variational_recurrent=hparams.decoder_variational_dropout[idx],\n",
        "                                            input_keep_prob=hparams.decoder_input_dropout[idx],\n",
        "                                            output_keep_prob=hparams.decoder_output_dropout[idx],\n",
        "                                            state_keep_prob=hparams.decoder_state_dropout[idx], seed=self.seed + idx)\n",
        "              return cell\n",
        "\n",
        "      if hparams.decoder_rnn_layers > 1:\n",
        "          cells = [build_cell(idx) for idx in range(hparams.decoder_rnn_layers)]\n",
        "          cell = rnn.MultiRNNCell(cells)\n",
        "      else:\n",
        "          cell = build_cell(0)\n",
        "\n",
        "      nest.assert_same_structure(encoder_state, cell.state_size)\n",
        "      predict_days = self.inp.predict_window\n",
        "      assert prediction_inputs.shape[1] == predict_days\n",
        "\n",
        "      # [batch_size, time, input_depth] -> [time, batch_size, input_depth]\n",
        "      inputs_by_time = tf.transpose(prediction_inputs, [1, 0, 2])\n",
        "\n",
        "      # Return raw outputs for RNN losses calculation\n",
        "      return_raw_outputs = self.hparams.decoder_stability_loss > 0.0 or self.hparams.decoder_activation_loss > 0.0\n",
        "\n",
        "      # Stop condition for decoding loop\n",
        "      def cond_fn(time, prev_output, prev_state, array_targets: tf.TensorArray, array_outputs: tf.TensorArray):\n",
        "          return time < predict_days\n",
        "\n",
        "      # FC projecting layer to get single predicted value from RNN output\n",
        "      def project_output(tensor):\n",
        "          \"\"\"\n",
        "          Every neuron in the network computes the same output, then they will \n",
        "          also all compute the same gradients during backpropagation and undergo \n",
        "          the exact same parameter updates. Since, the output has to be projected \n",
        "          to a single value and weights are to be very close to zero, initilizing \n",
        "          the weights to zero would be the best case here.\n",
        "          \n",
        "          Derivative of activation function has to be well-behaved at zero.\n",
        "          The main idea is to let the gradient be non zero and recover during\n",
        "          training eventually.\n",
        "          By default linear activation: a(z) = z\n",
        "          Others: 1) a(z) = tanh(z); a'(z) = sech^2(z)\n",
        "                  2) a(z) = sigmoid(z); a'(z) = a(z)(1 - a(z))\n",
        "          \"\"\"\n",
        "          return tf.layers.dense(tensor, 1, name='decoder_output_proj', \n",
        "                                 kernel_initializer=tf.zeros_initializer())\n",
        "\n",
        "      def loop_fn(time, prev_output, prev_state, array_targets: tf.TensorArray, array_outputs: tf.TensorArray):\n",
        "          \"\"\"\n",
        "          Main decoder loop\n",
        "          :param time: Day number\n",
        "          :param prev_output: Output(prediction) from previous step\n",
        "          :param prev_state: RNN state tensor from previous step\n",
        "          :param array_targets: Predictions, each step will append new value to this array\n",
        "          :param array_outputs: Raw RNN outputs (for regularization losses)\n",
        "          :return:\n",
        "          \"\"\"\n",
        "          # RNN inputs for current step\n",
        "          features = inputs_by_time[time]\n",
        "\n",
        "          next_input = tf.concat([prev_output, features], axis=1)\n",
        "\n",
        "          # Run RNN cell\n",
        "          output, state = cell(next_input, prev_state)\n",
        "          # Make prediction from RNN outputs\n",
        "          projected_output = project_output(output)\n",
        "          # Append step results to the buffer arrays\n",
        "          if return_raw_outputs:\n",
        "              array_outputs = array_outputs.write(time, output)\n",
        "          array_targets = array_targets.write(time, projected_output)\n",
        "          # Increment time and return\n",
        "          return time + 1, projected_output, state, array_targets, array_outputs\n",
        "\n",
        "      # Initial values for loop\n",
        "      loop_init = [tf.constant(0, dtype=tf.int32),\n",
        "                   tf.expand_dims(previous_y, -1),\n",
        "                   encoder_state,\n",
        "                   tf.TensorArray(dtype=tf.float32, size=predict_days),\n",
        "                   tf.TensorArray(dtype=tf.float32, size=predict_days) if return_raw_outputs else tf.constant(0)]\n",
        "      # Run the loop\n",
        "      _, _, _, targets_ta, outputs_ta = tf.while_loop(cond_fn, loop_fn, loop_init)\n",
        "\n",
        "      # Get final tensors from buffer arrays\n",
        "      targets = targets_ta.stack()\n",
        "      # [time, batch_size, 1] -> [time, batch_size]\n",
        "      #targets = tf.squeeze(targets, axis=-1)\n",
        "      raw_outputs = outputs_ta.stack() if return_raw_outputs else None\n",
        "      return targets, raw_outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xleAskOyLDSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.training as training\n",
        "\n",
        "params = dict(\n",
        "    epochs = 100,\n",
        "    batch_size=256,\n",
        "    train_window=24,\n",
        "    predict_window=48,\n",
        "    train_skip_first=0,\n",
        "    seed = 1,\n",
        "    rnn_depth=138,\n",
        "    encoder_readout_dropout=0.6415488109353416,\n",
        "\n",
        "    encoder_rnn_layers=1,\n",
        "    decoder_rnn_layers=2,\n",
        "\n",
        "    decoder_input_dropout=[1.0, 1.0, 1.0],\n",
        "    decoder_output_dropout=[0.925, 0.925, 1.0],\n",
        "    decoder_state_dropout=[0.98, 0.98, 0.995],\n",
        "    decoder_variational_dropout=[False, False, False],\n",
        "    decoder_candidate_l2=0.0,\n",
        "    decoder_gates_l2=0.0,\n",
        "    gate_dropout=0.9275441207192259,\n",
        "    gate_activation='none',\n",
        "    encoder_dropout=0.0,\n",
        "    encoder_stability_loss=0.0,\n",
        "    encoder_activation_loss=1e-06,\n",
        "    decoder_stability_loss=0.0,\n",
        "    decoder_activation_loss=1e-06,\n",
        ")\n",
        "\n",
        "def build_hparams():\n",
        "  return training.HParams(**params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53k8_dPN3aDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2535
        },
        "outputId": "872f21f1-f01d-45f7-e5e7-61b12da06c86"
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Oct 17 10:24:20 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "hparams = build_hparams()\n",
        "batch_size = hparams.batch_size\n",
        "train_window = hparams.train_window\n",
        "predict_window = hparams.predict_window\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "tf.set_random_seed(5)\n",
        "avg_sgd =  None\n",
        "\n",
        "inp = VarFeeder.read_vars(\"data/vars\")\n",
        "pipe_train = InputPipe(inp, mode=ModelMode.TRAIN, batch_size=batch_size, \n",
        "                       n_epoch=hparams.epochs, verbose=False, \n",
        "                       train_window=train_window, predict_window=predict_window,\n",
        "                       rand_seed=hparams.seed)\n",
        "#        pipe_eval = InputPipe(inp, mode=ModelMode.EVAL, batch_size=batch_size, n_epoch=None, verbose=False,\n",
        "#                                     train_window=train_window,\n",
        "#                                     predict_window=predict_window,\n",
        "#                                     rand_seed=hparams.seed)\n",
        "train_model = Model(pipe_train, hparams, is_train=True, graph_prefix=None, \n",
        "                    asgd_decay=None, seed=hparams.seed)\n",
        "#        eval_model = Model(pipe_eval, hparams, is_train=False, seed=hparams.seed)\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=False))) as sess:\n",
        "    sess.run(init)\n",
        "    inp.restore(sess)\n",
        "    train_model.inp.init_iterator(sess)\n",
        "    while True:\n",
        "      try:\n",
        "        sess.run(train_model.train_op)\n",
        "        print(sess.run(train_model.mape)*100)\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        print('🎉')\n",
        "        break"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "22.561603784561157\n",
            "118.0014967918396\n",
            "63.424086570739746\n",
            "33.937862515449524\n",
            "47.071707248687744\n",
            "39.427363872528076\n",
            "12.170913070440292\n",
            "30.456337332725525\n",
            "36.214566230773926\n",
            "23.35747927427292\n",
            "9.815003722906113\n",
            "21.436353027820587\n",
            "22.335951030254364\n",
            "13.893277943134308\n",
            "7.720848172903061\n",
            "10.61229407787323\n",
            "12.703743577003479\n",
            "8.20547565817833\n",
            "5.870617926120758\n",
            "8.757004886865616\n",
            "7.727976888418198\n",
            "4.73097562789917\n",
            "6.346043199300766\n",
            "7.27367177605629\n",
            "5.07335402071476\n",
            "3.1960662454366684\n",
            "5.363393947482109\n",
            "5.488908290863037\n",
            "3.3925075083971024\n",
            "3.2247141003608704\n",
            "4.312964156270027\n",
            "3.6414705216884613\n",
            "2.214333228766918\n",
            "3.4994449466466904\n",
            "3.602699562907219\n",
            "3.1339436769485474\n",
            "2.146577276289463\n",
            "2.7112217620015144\n",
            "2.721874415874481\n",
            "1.9281461834907532\n",
            "2.4371379986405373\n",
            "2.887841872870922\n",
            "2.320338785648346\n",
            "2.3897672072052956\n",
            "2.7060726657509804\n",
            "2.4971747770905495\n",
            "2.5316400453448296\n",
            "2.445630729198456\n",
            "2.2210871800780296\n",
            "2.7314065024256706\n",
            "2.2908858954906464\n",
            "1.6513548791408539\n",
            "1.9946400076150894\n",
            "2.3273147642612457\n",
            "1.8199605867266655\n",
            "1.5128004364669323\n",
            "1.610393077135086\n",
            "1.5604086220264435\n",
            "1.4862552285194397\n",
            "1.8959850072860718\n",
            "2.745642699301243\n",
            "2.7847522869706154\n",
            "1.641537994146347\n",
            "1.9669605419039726\n",
            "2.090630494058132\n",
            "2.508714981377125\n",
            "2.1463200449943542\n",
            "1.6494473442435265\n",
            "3.2450001686811447\n",
            "1.7406689003109932\n",
            "2.492530085146427\n",
            "2.0912647247314453\n",
            "1.9796157255768776\n",
            "1.7143560573458672\n",
            "2.816944569349289\n",
            "1.6352973878383636\n",
            "1.6094589605927467\n",
            "1.8798047676682472\n",
            "2.096877433359623\n",
            "1.5119089744985104\n",
            "1.358771976083517\n",
            "1.808297075331211\n",
            "2.134138159453869\n",
            "1.563032902777195\n",
            "1.3351358473300934\n",
            "1.6541281715035439\n",
            "2.0835673436522484\n",
            "1.4847171492874622\n",
            "1.414075493812561\n",
            "1.7314311116933823\n",
            "1.7589038237929344\n",
            "1.3922267593443394\n",
            "1.3461331836879253\n",
            "1.591763086616993\n",
            "1.6090111806988716\n",
            "1.4990165829658508\n",
            "1.308902632445097\n",
            "1.3576448895037174\n",
            "1.3816009275615215\n",
            "1.3131665997207165\n",
            "1.4105290174484253\n",
            "1.4481239020824432\n",
            "1.3887133449316025\n",
            "1.4127501286566257\n",
            "1.4692326076328754\n",
            "1.3563991524279118\n",
            "1.27722704783082\n",
            "1.376101840287447\n",
            "1.6830040141940117\n",
            "1.6652928665280342\n",
            "1.3818345963954926\n",
            "1.3330253772437572\n",
            "1.5778576955199242\n",
            "1.7869733273983002\n",
            "1.4561383984982967\n",
            "1.3774700462818146\n",
            "1.4764361083507538\n",
            "1.6053775325417519\n",
            "1.5169749967753887\n",
            "1.358900498598814\n",
            "1.3812396675348282\n",
            "1.4467095024883747\n",
            "1.4562872238457203\n",
            "1.5728900209069252\n",
            "1.502932608127594\n",
            "1.348190288990736\n",
            "1.2725855223834515\n",
            "1.404661312699318\n",
            "1.5548435039818287\n",
            "1.4999528415501118\n",
            "1.4019750989973545\n",
            "1.3333131559193134\n",
            "1.3330747373402119\n",
            "1.5930494293570518\n",
            "1.6099514439702034\n",
            "1.4016389846801758\n",
            "1.2689932249486446\n",
            "1.370879728347063\n",
            "1.5188992954790592\n",
            "1.639127917587757\n",
            "1.416288036853075\n",
            "1.3303245417773724\n",
            "1.5933148562908173\n",
            "🎉\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}