{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elf_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshkumar/elf/blob/master/elf_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "74JLaETpMp5A",
        "colab_type": "code",
        "outputId": "c06c33fb-2936-4e0a-a5d0-454a13cab87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L4sTGwL8zyEG",
        "colab_type": "code",
        "outputId": "dcb6713c-4036-47b2-ebdd-9e2a3e408720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-kmVZqCe6faX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp \"./drive/My Drive/data.zip\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bxu8d4Pe8xa2",
        "colab_type": "code",
        "outputId": "e7407a3d-2dd2-4b4b-baa4-23c159c0c654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3612
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip ./data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./data.zip\n",
            "   creating: data/\n",
            "   creating: data/VIC/\n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201501_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201711_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201710_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201612_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201706_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201707_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201604_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201605_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201510_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201511_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201701_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201609_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201608_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201603_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201602_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201507_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201506_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201607_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201606_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201705_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201704_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201509_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201508_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201610_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201611_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201503_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201502_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201712_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201504_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201505_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201601_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201708_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201709_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201512_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201702_VIC1.csv  \n",
            "  inflating: data/VIC/PRICE_AND_DEMAND_201703_VIC1.csv  \n",
            "   creating: data/vars/\n",
            "  inflating: data/vars/.DS_Store     \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/data/\n",
            "   creating: __MACOSX/data/vars/\n",
            "  inflating: __MACOSX/data/vars/._.DS_Store  \n",
            "  inflating: data/vars/feeder_meta.pkl  \n",
            "  inflating: data/vars/feeder.cpt.index  \n",
            "  inflating: data/vars/feeder.cpt.data-00000-of-00001  \n",
            "  inflating: data/.DS_Store          \n",
            "  inflating: data/all.pkl            \n",
            "   creating: data/SA/\n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201705_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201502_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201512_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201606_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201503_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201607_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201704_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201605_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201501_SA1.csv  \n",
            "   creating: __MACOSX/data/SA/\n",
            "  inflating: __MACOSX/data/SA/._PRICE_AND_DEMAND_201501_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201511_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201706_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201508_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201707_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201509_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201604_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201510_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201601_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201611_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201505_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201702_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201712_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201608_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201703_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201609_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201610_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201504_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201701_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201711_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201708_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201506_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201602_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201612_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201709_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201507_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201603_SA1.csv  \n",
            "  inflating: data/SA/PRICE_AND_DEMAND_201710_SA1.csv  \n",
            "   creating: data/QLD/\n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201504_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201505_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201708_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201709_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201601_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201512_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201702_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201703_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201607_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201606_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201705_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201704_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201509_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201508_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201610_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201611_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201503_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201502_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201712_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201510_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201511_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201609_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201608_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201701_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201603_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201602_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201507_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201506_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201501_QLD1.csv  \n",
            "   creating: __MACOSX/data/QLD/\n",
            "  inflating: __MACOSX/data/QLD/._PRICE_AND_DEMAND_201501_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201711_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201710_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201612_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201706_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201707_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201604_QLD1.csv  \n",
            "  inflating: data/QLD/PRICE_AND_DEMAND_201605_QLD1.csv  \n",
            "   creating: data/NSW/\n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201506_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201507_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201608_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201609_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201701_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201511_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201510_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201602_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201603_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201707_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201706_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201605_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201604_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201710_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201711_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201501_NSW1.csv  \n",
            "   creating: __MACOSX/data/NSW/\n",
            "  inflating: __MACOSX/data/NSW/._PRICE_AND_DEMAND_201501_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201612_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201709_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201708_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201601_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201703_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201702_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201512_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201505_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201504_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201611_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201610_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201508_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201509_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201712_NSW1.csv  \n",
            "  inflating: __MACOSX/data/NSW/._PRICE_AND_DEMAND_201712_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201502_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201503_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201606_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201607_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201704_NSW1.csv  \n",
            "  inflating: data/NSW/PRICE_AND_DEMAND_201705_NSW1.csv  \n",
            "   creating: data/TAS/\n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201602_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201603_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201701_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201608_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201609_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201511_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201510_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201506_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201507_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201612_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201710_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201711_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201501_TAS1.csv  \n",
            "   creating: __MACOSX/data/TAS/\n",
            "  inflating: __MACOSX/data/TAS/._PRICE_AND_DEMAND_201501_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201605_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201604_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201707_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201706_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201505_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201504_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201703_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201702_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201512_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201601_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201709_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201708_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201704_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201705_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201606_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201607_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201712_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201502_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201503_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201611_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201610_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201508_TAS1.csv  \n",
            "  inflating: data/TAS/PRICE_AND_DEMAND_201509_TAS1.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AxMVqA62NvXM",
        "colab_type": "code",
        "outputId": "cb561110-f747-4801-a226-cea1ba0a462b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 12.5 GB  | Proc size: 689.1 MB\n",
            "GPU RAM Free: 11325MB | Used: 116MB | Util   1% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MdLtdR8KMHMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 19:05:59 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from collections import UserList, UserDict\n",
        "from typing import Union, Iterable, Tuple, Dict, Any\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os.path\n",
        "\n",
        "def _meta_file(path):\n",
        "    return os.path.join(path, 'feeder_meta.pkl')\n",
        "\n",
        "\n",
        "class VarFeeder:\n",
        "    \"\"\"\n",
        "    Builds temporary TF graph, injects variables into, and saves variables to TF checkpoint.\n",
        "    In a train time, variables can be built by build_vars() and content restored by FeederVars.restore()\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str,\n",
        "                 tensor_vars: Dict[str, Union[pd.DataFrame, pd.Series, np.ndarray]] = None,\n",
        "                 plain_vars: Dict[str, Any] = None):\n",
        "        \"\"\"\n",
        "        :param path: dir to store data\n",
        "        :param tensor_vars: Variables to save as Tensors (pandas DataFrames/Series or numpy arrays)\n",
        "        :param plain_vars: Variables to save as Python objects\n",
        "        \"\"\"\n",
        "        tensor_vars = tensor_vars or dict()\n",
        "\n",
        "        def get_values(v):\n",
        "            v = v.values if hasattr(v, 'values') else v\n",
        "            if not isinstance(v, np.ndarray):\n",
        "                v = np.array(v)\n",
        "            if v.dtype == np.float64:\n",
        "                v = v.astype(np.float32)\n",
        "            return v\n",
        "\n",
        "        values = [get_values(var) for var in tensor_vars.values()]\n",
        "\n",
        "        self.shapes = [var.shape for var in values]\n",
        "        self.dtypes = [v.dtype for v in values]\n",
        "        self.names = list(tensor_vars.keys())\n",
        "        self.path = path\n",
        "        self.plain_vars = plain_vars\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "        with open(_meta_file(path), mode='wb') as file:\n",
        "            pickle.dump(self, file)\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            tensor_vars = self._build_vars()\n",
        "            placeholders = [tf.placeholder(tf.as_dtype(dtype), shape=shape) for dtype, shape in\n",
        "                            zip(self.dtypes, self.shapes)]\n",
        "            assigners = [tensor_var.assign(placeholder) for tensor_var, placeholder in\n",
        "                         zip(tensor_vars, placeholders)]\n",
        "            feed = {ph: v for ph, v in zip(placeholders, values)}\n",
        "            saver = tf.train.Saver(self._var_dict(tensor_vars), max_to_keep=1)\n",
        "            init = tf.global_variables_initializer()\n",
        "\n",
        "            with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\n",
        "                sess.run(init)\n",
        "                sess.run(assigners, feed_dict=feed)\n",
        "                save_path = os.path.join(path, 'feeder.cpt')\n",
        "                saver.save(sess, save_path, write_meta_graph=False, write_state=False)\n",
        "\n",
        "    def _var_dict(self, variables):\n",
        "        return {name: var for name, var in zip(self.names, variables)}\n",
        "\n",
        "    def _build_vars(self):\n",
        "        def make_tensor(shape, dtype, name):\n",
        "            tf_type = tf.as_dtype(dtype)\n",
        "            if tf_type == tf.string:\n",
        "                empty = ''\n",
        "            elif tf_type == tf.bool:\n",
        "                empty = False\n",
        "            else:\n",
        "                empty = 0\n",
        "            init = tf.constant(empty, shape=shape, dtype=tf_type)\n",
        "            return tf.get_local_variable(name=name, initializer=init, dtype=tf_type)\n",
        "\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            with tf.name_scope('feeder_vars'):\n",
        "                return [make_tensor(shape, dtype, name) for shape, dtype, name in\n",
        "                        zip(self.shapes, self.dtypes, self.names)]\n",
        "\n",
        "    def create_vars(self):\n",
        "        \"\"\"\n",
        "        Builds variable list to use in current graph. Should be called during graph building stage\n",
        "        :return: variable list with additional restore and create_saver methods\n",
        "        \"\"\"\n",
        "        return FeederVars(self._var_dict(self._build_vars()), self.plain_vars, self.path)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_vars(path):\n",
        "        with open(_meta_file(path), mode='rb') as file:\n",
        "            feeder = pickle.load(file)\n",
        "        assert feeder.path == path\n",
        "        return feeder.create_vars()\n",
        "\n",
        "\n",
        "class FeederVars(UserDict):\n",
        "    def __init__(self, tensors: dict, plain_vars: dict, path):\n",
        "        variables = dict(tensors)\n",
        "        if plain_vars:\n",
        "            variables.update(plain_vars)\n",
        "        super().__init__(variables)\n",
        "        self.path = path\n",
        "        self.saver = tf.train.Saver(tensors, name='varfeeder_saver')\n",
        "        for var in variables:\n",
        "            if var not in self.__dict__:\n",
        "                self.__dict__[var] = variables[var]\n",
        "\n",
        "    def restore(self, session):\n",
        "        \"\"\"\n",
        "        Restores variable content\n",
        "        :param session: current session\n",
        "        :return: variable list\n",
        "        \"\"\"\n",
        "        self.saver.restore(session, os.path.join(self.path, 'feeder.cpt'))\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Vw4L1iCMWAh",
        "colab_type": "code",
        "outputId": "b2e84416-2e04-4800-b1f1-0ce9c0794197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Oct  4 12:02:32 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def read_all() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads source data for training/prediction\n",
        "    \"\"\"\n",
        "    # Path to cached data\n",
        "    path = os.path.join('data', 'all.pkl')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_pickle(path)\n",
        "        return df\n",
        "    else:\n",
        "        state = {0: 'NSW', 1: 'QLD', 2: 'SA', 3: 'TAS', 4: 'VIC'}\n",
        "        year = {0: '2015', 1: '2016', 2: '2017'}\n",
        "        \n",
        "        df_nsw = pd.DataFrame()\n",
        "        df_qld = pd.DataFrame()\n",
        "        df_sa = pd.DataFrame()\n",
        "        df_tas = pd.DataFrame()\n",
        "        df_vic = pd.DataFrame()\n",
        "        \n",
        "        df_all = pd.DataFrame()\n",
        "        \n",
        "        df = {'NSW': df_nsw, 'QLD': df_qld, 'SA': df_sa, 'TAS': df_tas, 'VIC': df_vic}\n",
        "        \n",
        "        for st in state.values():\n",
        "            for ye in year.values():\n",
        "                for mn in range(1,13):\n",
        "                    if mn < 10:            \n",
        "                        dataset = pd.read_csv('./data/' + st + '/PRICE_AND_DEMAND_' + ye + '0' + str(mn) +'_' + st + '1.csv')\n",
        "                    else:\n",
        "                        dataset = pd.read_csv('./data/' + st + '/PRICE_AND_DEMAND_' + ye + str(mn) +'_' + st + '1.csv')\n",
        "                    df[st] = df[st].append(dataset.iloc[:,:3])\n",
        "            df_all = df_all.append(df[st].iloc[:,:])\n",
        "        df_all = df_all.set_index(['REGION', 'SETTLEMENTDATE'])\n",
        "        df_all.to_pickle(path)\n",
        "        return df_all\n",
        "    \n",
        "def read_x(state, start, end) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Gets source data from start to end date. Any date can be None\n",
        "    \"\"\"\n",
        "    def read_state(state):\n",
        "        df = read_all()\n",
        "        return df.loc[state]\n",
        "    df = read_state(state)\n",
        "    if start and end:\n",
        "        return df.loc[start:end]\n",
        "    elif end:\n",
        "        return df.loc[:end]\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def single_autocorr(series, lag):\n",
        "    \"\"\"\n",
        "    Autocorrelation for single data series\n",
        "    :param series: traffic series\n",
        "    :param lag: lag, days\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    s1 = series[lag:]\n",
        "    s2 = series[:-lag]\n",
        "    ms1 = np.mean(s1)\n",
        "    ms2 = np.mean(s2)\n",
        "    ds1 = s1 - ms1\n",
        "    ds2 = s2 - ms2\n",
        "    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n",
        "    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n",
        "\n",
        "def batch_autocorr(series, lag, starts, ends, backoffset=0):\n",
        "    \"\"\"\n",
        "    Calculate autocorrelation for batch (many time series at once)\n",
        "    :param data: Time series\n",
        "    :param lag: Autocorrelation lag\n",
        "    :param starts: Start index for series\n",
        "    :param ends: End index for series\n",
        "    :param backoffset: Offset from the series end, days.\n",
        "    :return: autocorrelation, shape [n_series].\n",
        "    \"\"\"\n",
        "    n_series = series.shape[0]\n",
        "    max_end = n_series - backoffset\n",
        "    \n",
        "    end = min(ends, max_end)\n",
        "    series = series[starts:end]\n",
        "    c_365 = single_autocorr(series, lag)\n",
        "    c_364 = single_autocorr(series, lag-1)\n",
        "    c_366 = single_autocorr(series, lag+1)\n",
        "    # Average value between exact lag and two nearest neighborhs for smoothness\n",
        "    corr = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n",
        "    return corr #, support\n",
        "\n",
        "def prepare_data(start, end, state) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(series)\n",
        "    :param start: start date of effective time interval, can be None to start from beginning\n",
        "    :param end: end date of effective time interval, can be None to return all data\n",
        "    :param state: state of series\n",
        "    :return: log1p(series)\n",
        "    \"\"\"\n",
        "    df = read_x(state, start, end)\n",
        "    return np.log1p(df.fillna(0))\n",
        "\n",
        "def normalize(values: np.ndarray):\n",
        "    return (values - values.mean()) / np.std(values)\n",
        "\n",
        "def lag_indexes(begin, end) -> List[pd.Series]:\n",
        "    \"\"\"\n",
        "    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n",
        "    :param begin: start of date range\n",
        "    :param end: end of date range\n",
        "    :return: List of 4 Series, one for each lag. For each Series, index is date in range(begin, end), value is an index\n",
        "     of target (lagged) date in a same Series. If target date is out of (begin,end) range, index is -1\n",
        "    \"\"\"\n",
        "    dr = pd.date_range(begin, end, freq='0.5H')\n",
        "    # key is date, value is day index\n",
        "    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n",
        "\n",
        "    def lag(offset):\n",
        "        dates = dr - offset\n",
        "        return pd.Series(data=base_index.loc[dates].fillna(-1).astype(np.int32).values, index=dr)\n",
        "\n",
        "    return [lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]\n",
        "\n",
        "def run():\n",
        "    \n",
        "    data_dir = \"data/vars\"\n",
        "    state = \"NSW1\"\n",
        "    add_end = 0\n",
        "    start = \"2015/01/01 00:30:00\"\n",
        "    end = \"2018/01/01 00:00:00\"\n",
        "    corr_backoffset = 0\n",
        "    \n",
        "    # Get the data\n",
        "    df = prepare_data(start, end, state)\n",
        "    \n",
        "    # Our working date range\n",
        "    data_start, data_end = pd.to_datetime(df.first_valid_index()), pd.to_datetime(df.last_valid_index())\n",
        "    \n",
        "    # We have to project some date-dependent features (day of week, etc) to the future dates for prediction\n",
        "    features_end = data_end + pd.Timedelta(add_end/2, unit='h')\n",
        "    print(f\"start: {data_start}, end:{data_end}, features_end:{features_end}\")\n",
        "    \n",
        "    starts = 0\n",
        "    ends = df.shape[0]\n",
        "    \n",
        "    # Yearly(annual) autocorrelation\n",
        "    year_autocorr = batch_autocorr(df.values, 365, starts, ends, corr_backoffset)\n",
        "    \n",
        "    # Quarterly autocorrelation\n",
        "    quarter_autocorr = batch_autocorr(df.values, int(round(365.25/4)), starts, ends, corr_backoffset)\n",
        "    \n",
        "    # Make time-dependent features\n",
        "    features = pd.date_range(data_start, features_end, freq='0.5H')\n",
        "    #dow = normalize(features_days.dayofweek.values)\n",
        "    week_period = 7 / (2 * np.pi)\n",
        "    dow_norm = features.dayofweek.values / week_period\n",
        "    dow = np.stack([np.cos(dow_norm), np.sin(dow_norm)], axis=-1)\n",
        "    \n",
        "     # Assemble indices for quarterly lagged data\n",
        "    lagged_ix = np.stack(lag_indexes(data_start, features_end), axis=-1)\n",
        "    \n",
        "    # Assemble final output\n",
        "    tensors = dict(\n",
        "        hits=df,\n",
        "        lagged_ix=lagged_ix,\n",
        "        page_ix=df.index.values,\n",
        "        dow=dow\n",
        "        )\n",
        "    plain = dict(\n",
        "        features=len(features),\n",
        "        year_autocorr=year_autocorr,\n",
        "        quarter_autocorr=quarter_autocorr,\n",
        "        data_size=df.shape[0],\n",
        "        data_start=data_start,\n",
        "        data_end=data_end,\n",
        "        features_end=features_end\n",
        "        )\n",
        "\n",
        "    # Store data to the disk\n",
        "    VarFeeder(data_dir, tensors, plain)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    python make_features.py data/vars --add_end=0 --start=\"2015/01/01 00:30:00\" --end=\"2018/01/01 00:00:00\"\n",
        "    \"\"\"\n",
        "    run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start: 2015-01-01 00:30:00, end:2018-01-01 00:00:00, features_end:2018-01-01 00:00:00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zJABrGwVhIPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 20:09:16 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from enum import Enum\n",
        "import tensorflow as tf\n",
        "\n",
        "class ModelMode(Enum):\n",
        "  TRAIN = 0,\n",
        "  EVAL = 1,\n",
        "  PREDICT = 2\n",
        "\n",
        "class InputPipe:\n",
        "  def __init__(self, inp: VarFeeder, mode: ModelMode, n_epoch=None,\n",
        "               batch_size=32, runs_in_burst=1, verbose=True, predict_window=48, train_window=36, back_offset=0,\n",
        "               train_skip_first=0, rand_seed=None, repetition = 1):\n",
        "      \"\"\"\n",
        "      Create data preprocessing pipeline\n",
        "      :param inp: Raw input data\n",
        "      :param features: Features tensors (subset of data in inp)\n",
        "      :param mode: Train/Predict/Eval mode selector\n",
        "      :param n_epoch: Number of epochs. Generates endless data stream if None\n",
        "      :param batch_size: (https://www.tensorflow.org/guide/datasets#batching_dataset_elements)\n",
        "      :param n_splits: number of time series split\n",
        "      :param runs_in_burst: How many batches can be consumed at short time interval (burst). Multiplicator for prefetch() (https://www.tensorflow.org/performance/datasets_performance)\n",
        "      :param verbose: Print additional information during graph construction\n",
        "      :param predict_window: Number of days to predict\n",
        "      :param train_window: Use train_window days for traning\n",
        "      :param back_offset: Don't use back_offset days at the end of timeseries\n",
        "      :param train_skip_first: Don't use train_skip_first days at the beginning of timeseries\n",
        "      :param rand_seed:\n",
        "      :param repetition: repeat the dataset repetition times\n",
        "      \"\"\"\n",
        "      self.inp = inp\n",
        "      self.batch_size = batch_size\n",
        "      self.rand_seed = rand_seed\n",
        "      self.back_offset = back_offset\n",
        "      self.start_offset = train_skip_first\n",
        "      self.train_window = train_window\n",
        "      self.predict_window = predict_window\n",
        "      self.mode = mode\n",
        "      self.verbose = verbose\n",
        "      \n",
        "      list_half_hourly_load = self.inp.hits[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      list_half_hourly_load = list_half_hourly_load / tf.linalg.norm(list_half_hourly_load)\n",
        "      matrix_hits = list_half_hourly_load[:-(list_half_hourly_load.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_hits = tf.reshape(matrix_hits, [-1, (train_window + predict_window)])\n",
        "      \n",
        "      matrix_dow = self.inp.dow[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      matrix_dow = matrix_dow[:-(matrix_dow.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_dow = tf.reshape(matrix_dow, [-1, (train_window + predict_window), 2])\n",
        "      \n",
        "      # Convert -1 to 0 for gather(), it don't accept anything exotic\n",
        "      lags_indices = tf.maximum(self.inp.lagged_ix, 0)\n",
        "      # Translate lag indexes to hit values\n",
        "      lagged_hit = tf.gather(list_half_hourly_load, lags_indices)\n",
        "      lagged_hit = lagged_hit / tf.linalg.norm(lagged_hit)\n",
        "      lagged_hit = lagged_hit[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      matrix_lagged_hit = lagged_hit[:-(lagged_hit.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_lagged_hit = tf.reshape(matrix_lagged_hit, [-1, (train_window + predict_window), 4])\n",
        "      \n",
        "      x_hits = matrix_hits[:, :train_window]\n",
        "      y_hits = matrix_hits[:, train_window:]\n",
        "      \n",
        "      x_dow = matrix_dow[:, :train_window]\n",
        "      y_dow = matrix_dow[:, train_window:]\n",
        "      \n",
        "      x_lagged_hit = matrix_lagged_hit[:, :train_window]\n",
        "      y_lagged_hit = matrix_lagged_hit[:, train_window:]\n",
        "\n",
        "      def tileFeatures(window):\n",
        "        # [n_features] => [window, n_features]\n",
        "        flat_features = tf.stack([self.inp.year_autocorr, self.inp.quarter_autocorr], axis=0 )\n",
        "        features = tf.expand_dims(flat_features, axis=0)\n",
        "        features = tf.tile(features, [window,1])\n",
        "\n",
        "        # [train_window, n_features] => [size, window, n_features]\n",
        "        features = tf.expand_dims(features, axis=0)\n",
        "        features = tf.tile(features, [x_hits.shape[0].value, 1,1])\n",
        "        return features\n",
        "      \n",
        "      x_features = tf.concat((tf.reshape(x_hits, [x_hits.shape[0].value,train_window,1]),\n",
        "                                   x_dow, x_lagged_hit, \n",
        "                                   tileFeatures(train_window) \n",
        "                                   ), axis = 2)\n",
        "      \n",
        "      y_features = tf.concat((y_dow, \n",
        "                              y_lagged_hit,\n",
        "                              tileFeatures(predict_window)\n",
        "                             ), axis = 2)\n",
        "      \n",
        "      y_labels = tf.reshape(y_hits, [y_hits.shape[0].value, predict_window, 1])\n",
        "      \n",
        "      # Assume that each row of `features` corresponds to the same row as `labels`.\n",
        "      assert x_features.shape[0].value == y_labels.shape[0].value\n",
        "      DATASET_SIZE = x_features.shape[0].value\n",
        "      \n",
        "      dataset = tf.data.Dataset.from_tensor_slices((x_features, y_features, y_labels))\n",
        "      \n",
        "      train_size = int(0.7 * DATASET_SIZE)\n",
        "      val_size = int(0.15 * DATASET_SIZE)\n",
        "      test_size = int(0.15 * DATASET_SIZE)\n",
        "\n",
        "      train_dataset = dataset.take(train_size)\n",
        "      test_dataset = dataset.skip(train_size)\n",
        "      val_dataset = test_dataset.skip(val_size)\n",
        "      test_dataset = test_dataset.take(test_size)\n",
        "\n",
        "      cutter = {ModelMode.TRAIN: train_dataset, ModelMode.EVAL: val_dataset, ModelMode.PREDICT: test_dataset}\n",
        "      \n",
        "      # [Other transformations on `dataset`...]\n",
        "      dataset = cutter[mode]\n",
        "      dataset = dataset.repeat(repetition)  # Repeat the input dataset.\n",
        "      dataset = dataset.batch(batch_size)\n",
        "      dataset = dataset.prefetch(runs_in_burst * 2)\n",
        "\n",
        "      self.iterator = dataset.make_initializable_iterator()\n",
        "      self.x_feature, self.y_feature, self.y_true = self.iterator.get_next()\n",
        "      self.encoder_features_depth = self.x_feature.shape[2].value\n",
        "\n",
        "  def load_vars(self, session):\n",
        "      self.inp.restore(session)\n",
        "\n",
        "  def init_iterator(self, session):\n",
        "      session.run(self.iterator.initializer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xleAskOyLDSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.training as training\n",
        "\n",
        "params = dict(\n",
        "    epochs = 20,\n",
        "    repetition = 2000,\n",
        "    batch_size=256,\n",
        "    train_window=24,\n",
        "    predict_window=48,\n",
        "    train_skip_first=0,\n",
        "    seed = 1,\n",
        "    rnn_depth=138,\n",
        "    encoder_readout_dropout=0.6415488109353416,\n",
        "\n",
        "    encoder_rnn_layers=1,\n",
        "    decoder_rnn_layers=2,\n",
        "\n",
        "    decoder_input_dropout=[1.0, 1.0, 1.0],\n",
        "    decoder_output_dropout=[0.925, 0.925, 1.0],\n",
        "    decoder_state_dropout=[0.98, 0.98, 0.995],\n",
        "    decoder_variational_dropout=[False, False, False],\n",
        "    decoder_candidate_l2=0.0,\n",
        "    decoder_gates_l2=0.0,\n",
        "    gate_dropout=0.9275441207192259,\n",
        "    gate_activation='none',\n",
        "    encoder_dropout=0.0,\n",
        "    encoder_stability_loss=0.0,\n",
        "    encoder_activation_loss=1e-06,\n",
        "    decoder_stability_loss=0.0,\n",
        "    decoder_activation_loss=1e-06,\n",
        ")\n",
        "\n",
        "def build_hparams():\n",
        "  return training.HParams(**params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJaU2ym4oAap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Model(inp, pipe, mode, hparams, init):\n",
        "\n",
        "  def build_rnn():\n",
        "    return tf.contrib.cudnn_rnn.CudnnGRU(num_layers=hparams.encoder_rnn_layers, num_units=hparams.rnn_depth,\n",
        "               direction='unidirectional',\n",
        "               dropout=hparams.encoder_dropout, seed=hparams.seed)\n",
        "  \n",
        "  def mape(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + 1e-10))))\n",
        "\n",
        "  cuda_model = build_rnn()\n",
        "\n",
        "  rnn_time_input = pipe.x_feature\n",
        "  # rnn_time_input = tf.placeholder(tf.float32, shape=(None, 24, 9))\n",
        "\n",
        "  rnn_out, (rnn_state,) = cuda_model(inputs=rnn_time_input)\n",
        "\n",
        "  learning_rate = 0.001   #small learning rate so we don't overshoot the minimum\n",
        "\n",
        "  fc_input = tf.reshape(rnn_out, [-1, hparams.train_window*hparams.rnn_depth])\n",
        "  fc_outputs = tf.layers.dense(fc_input, hparams.predict_window, \n",
        "                               kernel_initializer=init,\n",
        "                              activation = tf.tanh)       #specify the type of layer (dense)\n",
        "  outputs = tf.reshape(fc_outputs, [-1, hparams.predict_window, 1])          #shape of results\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - pipe.y_true))    #define the cost function which evaluates the quality of our model\n",
        "  mape_loss = mape(pipe.y_true, outputs)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)          #gradient descent method\n",
        "  training_op = optimizer.minimize(loss)          #train the result of the application of the cost_function                                 \n",
        "\n",
        "  model_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "  # Make EMA object and update interal variables after optimization step\n",
        "  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
        "  with tf.control_dependencies([training_op]):\n",
        "      train_op = ema.apply(model_vars)\n",
        "\n",
        "  \"\"\"\n",
        "  model_vars = [_, fc_weights, fc_biases]\n",
        "  we want to apply moving average only to the fc_weights \n",
        "  \"\"\"\n",
        "  # Transfer EMA values to original variables\n",
        "  retrieve_ema_weights_op = tf.group([tf.assign(model_vars[1], ema.average(model_vars[1]))])\n",
        "\n",
        "  return train_op, mape_loss, retrieve_ema_weights_op\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eZSydLd8wT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code for compairing different initialization tehniques."
      ]
    },
    {
      "metadata": {
        "id": "24LhzDYat2bK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# inits = {\n",
        "#          'Xa': tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=\"FAN_AVG\",uniform=True), \n",
        "#          'He': tf.keras.initializers.he_normal(),\n",
        "#          'Identity': tf.keras.initializers.Identity(),\n",
        "#          'ZI': tf.zeros_initializer(),\n",
        "#          'Linear': None,\n",
        "# #          'Norm': tf.initializers.truncated_normal()\n",
        "#         }\n",
        "\n",
        "# def mape(y_true, y_pred):\n",
        "#   return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + 1e-10))))\n",
        "\n",
        "# hparams = build_hparams()\n",
        "# tf.reset_default_graph()\n",
        "# inp = VarFeeder.read_vars(\"data/vars\")\n",
        "# pipe_train = InputPipe(inp, mode=ModelMode.TRAIN, batch_size=hparams.batch_size, \n",
        "#                        n_epoch=hparams.epochs, verbose=False, \n",
        "#                        train_window=hparams.train_window, predict_window=hparams.predict_window,\n",
        "#                        rand_seed=hparams.seed)\n",
        "\n",
        "# def build_rnn():\n",
        "#   return tf.contrib.cudnn_rnn.CudnnGRU(num_layers=hparams.encoder_rnn_layers, num_units=hparams.rnn_depth,\n",
        "#              direction='unidirectional',\n",
        "#              dropout=hparams.encoder_dropout, seed=hparams.seed)\n",
        "\n",
        "# for init_name, init in inits.items():\n",
        "\n",
        "#   cuda_model = build_rnn()\n",
        "\n",
        "#   rnn_time_input = pipe_train.x_feature\n",
        "\n",
        "#   rnn_out, (rnn_state,) = cuda_model(inputs=rnn_time_input)\n",
        "\n",
        "#   learning_rate = 0.001   #small learning rate so we don't overshoot the minimum\n",
        "\n",
        "#   fc_input = tf.reshape(rnn_out, [-1, hparams.train_window*hparams.rnn_depth])\n",
        "#   fc_outputs = tf.layers.dense(fc_input, hparams.predict_window, \n",
        "#                                kernel_initializer=init,\n",
        "#                               activation = tf.tanh)       #specify the type of layer (dense)\n",
        "#   outputs = tf.reshape(fc_outputs, [-1, hparams.predict_window, 1])          #shape of results\n",
        "\n",
        "#   loss = tf.reduce_mean(tf.square(outputs - pipe_train.y_true))    #define the cost function which evaluates the quality of our model\n",
        "#   mape_loss = mape(pipe_train.y_true, outputs)\n",
        "#   optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)          #gradient descent method\n",
        "#   training_op = optimizer.minimize(loss)          #train the result of the application of the cost_function                                 \n",
        "\n",
        "#   model_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "#   # Make EMA object and update interal variables after optimization step\n",
        "#   ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
        "#   with tf.control_dependencies([training_op]):\n",
        "#       train_op = ema.apply(model_vars)\n",
        "\n",
        "#   \"\"\"\n",
        "#   model_vars = [_, fc_weights, fc_biases]\n",
        "#   we want to apply moving average only to the fc_weights \n",
        "#   \"\"\"\n",
        "#   # Transfer EMA values to original variables\n",
        "#   retrieve_ema_weights_op = tf.group([tf.assign(model_vars[1], ema.average(model_vars[1]))])\n",
        "\n",
        "#   init = tf.global_variables_initializer()           #initialize all the variables\n",
        "\n",
        "#   mape_noASGD = []\n",
        "#   mape_ASGD = []\n",
        "\n",
        "#   for ASGD in [True]:\n",
        "#     with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "#         init.run()\n",
        "#         inp.restore(sess)\n",
        "#         pipe_train.init_iterator(sess)\n",
        "\n",
        "#         for ep in range(hparams.epochs):\n",
        "#             _, error = sess.run([train_op, mape_loss])\n",
        "#             if not ASGD:\n",
        "#               mape_noASGD.append(error*100)\n",
        "\n",
        "#             if ASGD:\n",
        "#                 mape_ASGD.append(error*100)\n",
        "#                 #if error*100 < 10:\n",
        "#                 # Copy EMA values to weights\n",
        "#                 sess.run(retrieve_ema_weights_op)\n",
        "#             if ep % 100 == 0:\n",
        "#                 print(ep, \"MAPE:\", error*100)\n",
        "#     print(\"\\n\")\n",
        "\n",
        "#   if ASGD:\n",
        "#     f_name = \"mape_ASGD_\" + init_name + \"_.pkl\"\n",
        "#     with open(f_name, 'wb') as fp:\n",
        "#       pickle.dump(mape_ASGD, fp)\n",
        "#   else:\n",
        "#     f_name = \"mape_noASGD_\" + init_name + \"_.pkl\"\n",
        "#     with open(f_name, 'wb') as fp:\n",
        "#         pickle.dump(mape_noASGD, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqA7mj_CTfBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !cp ./mape* \"./drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aOO1jBxKagW0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below code generates plot for showing variance reduction technique. \n",
        "The code uses both the files , one with ASGD and one with not. \n",
        "To generate both the file change the for loop above to include both \"True\" and \"False\"\n"
      ]
    },
    {
      "metadata": {
        "id": "VYjxQezsA28A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for init in inits:\n",
        "#   f_name = \"mape_ASGD_\" + init + \"_.pkl\"\n",
        "#   with open (f_name, 'rb') as fp:\n",
        "#       mape_ASGD = pickle.load(fp)\n",
        "#   f_name = \"mape_noASGD_\" + init + \"_.pkl\"\n",
        "#   with open (f_name, 'rb') as fp:\n",
        "#       mape_noASGD = pickle.load(fp)\n",
        "\n",
        "#   plt.plot(range(hparams.epochs), mape_noASGD, color=\"#add8e6\", label=\"NO ASGD\")\n",
        "#   plt.plot(range(hparams.epochs), mape_ASGD, label=\"ASGD applied\")\n",
        "#   plt.xlim(250,500)\n",
        "#   plt.ylim(1,6)\n",
        "#   plt.xlabel('epochs')\n",
        "#   plt.ylabel('MAPE')\n",
        "#   plt.title(init)\n",
        "#   plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6SBUmeNY5fS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# inits = [\n",
        "#          'Xa', \n",
        "#          'He',\n",
        "#          'Identity',\n",
        "#          'ZI',\n",
        "#          'Linear',\n",
        "# #          'Norm'\n",
        "# ]\n",
        "\n",
        "# for init in inits:\n",
        "#   f_name = \"mape_ASGD_\" + init + \"_.pkl\"\n",
        "#   with open (f_name, 'rb') as fp:\n",
        "#       mape_ASGD = pickle.load(fp)\n",
        "#   plt.plot(range(hparams.epochs), mape_ASGD, label=init)\n",
        "# plt.xlim(50,200)\n",
        "# plt.ylim(0,100)\n",
        "# plt.xlabel('epochs')\n",
        "# plt.ylabel('MAPE')\n",
        "# plt.title(\"Model 1\")\n",
        "# plt.legend()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J2PUu639Jiu",
        "colab_type": "code",
        "outputId": "39ea9bc8-489d-4593-c866-8cc36c5443af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2448
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "hparams = build_hparams()\n",
        "\n",
        "# graph1 = tf.Graph()\n",
        "\n",
        "inits = {\n",
        "         'Xa_norm': tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=\"FAN_AVG\",uniform=True), \n",
        "         'He': tf.keras.initializers.he_normal(),\n",
        "         'Identity': tf.keras.initializers.Identity(),\n",
        "         'ZI': tf.zeros_initializer(),\n",
        "         'Xa_uniform': None,  # https://github.com/tensorflow/tensorflow/blob/2a0e004358f13d6ebe936ceab1b5e7d147606583/tensorflow/python/ops/init_ops.py#L1134\n",
        "        }\n",
        "\n",
        "for init_name, init in inits.items():\n",
        "  tf.reset_default_graph()\n",
        "  inp = VarFeeder.read_vars(\"data/vars\")\n",
        "\n",
        "  pipe_train = InputPipe(inp, mode=ModelMode.TRAIN, batch_size=hparams.batch_size, \n",
        "                     n_epoch=hparams.epochs, verbose=False, \n",
        "                     train_window=hparams.train_window, predict_window=hparams.predict_window,\n",
        "                     rand_seed=hparams.seed, repetition = hparams.repetition)\n",
        "  train_op, mape_loss, retrieve_ema_weights_op = Model(inp, pipe_train, ModelMode.TRAIN, hparams, init)\n",
        "\n",
        "  with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      inp.restore(sess)\n",
        "\n",
        "      print(\"Training the model with\",init_name,\"...\")\n",
        "      for ep in range(hparams.epochs):\n",
        "        pipe_train.init_iterator(sess)\n",
        "        while True:\n",
        "          try:\n",
        "            _, error = sess.run([train_op, mape_loss])\n",
        "            sess.run(retrieve_ema_weights_op)\n",
        "          except tf.errors.OutOfRangeError:\n",
        "            break\n",
        "        print(ep, \"MAPE:\", error*100)\n",
        "      print(\"Training done...\")\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "      # Save model state\n",
        "      print('Saving')\n",
        "      saver_path = f\"data/cpt/rnn/\" + init_name\n",
        "      if os.path.exists(saver_path):\n",
        "          shutil.rmtree(saver_path)\n",
        "      os.makedirs(saver_path)\n",
        "      saver = tf.train.Saver(max_to_keep=10, name='train_saver')\n",
        "      saver_file = f\"data/cpt/rnn/\" + init_name + \"/elf_rnn.ckpt\"\n",
        "      saver.save(sess, saver_file)\n",
        "      print('Ok')\n",
        "# !zip -r checkpoints_rnn.zip data/cpt/rnn/\n",
        "# !cp checkpoints_rnn.zip \"./drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "Training the model with Xa_norm ...\n",
            "0 MAPE: 11.257722973823547\n",
            "1 MAPE: 5.750232934951782\n",
            "2 MAPE: 5.689932405948639\n",
            "3 MAPE: 3.5730544477701187\n",
            "4 MAPE: 3.803638741374016\n",
            "5 MAPE: 2.7243439108133316\n",
            "6 MAPE: 4.658573865890503\n",
            "7 MAPE: 3.3056549727916718\n",
            "8 MAPE: 3.2538123428821564\n",
            "9 MAPE: 3.2661769539117813\n",
            "10 MAPE: 2.8554528951644897\n",
            "11 MAPE: 2.7223505079746246\n",
            "12 MAPE: 3.2246045768260956\n",
            "13 MAPE: 2.210008352994919\n",
            "14 MAPE: 2.387990802526474\n",
            "15 MAPE: 2.526988834142685\n",
            "16 MAPE: 2.826434373855591\n",
            "17 MAPE: 2.639254368841648\n",
            "18 MAPE: 2.6264499872922897\n",
            "19 MAPE: 2.785763144493103\n",
            "Training done...\n",
            "\n",
            "\n",
            "\n",
            "Saving\n",
            "Ok\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "Training the model with He ...\n",
            "0 MAPE: 10.628367215394974\n",
            "1 MAPE: 11.81752011179924\n",
            "2 MAPE: 4.249089956283569\n",
            "3 MAPE: 4.462365806102753\n",
            "4 MAPE: 4.300985112786293\n",
            "5 MAPE: 2.7004702016711235\n",
            "6 MAPE: 4.423727467656136\n",
            "7 MAPE: 4.497797414660454\n",
            "8 MAPE: 3.61788310110569\n",
            "9 MAPE: 3.669453412294388\n",
            "10 MAPE: 3.2153531908988953\n",
            "11 MAPE: 3.3368032425642014\n",
            "12 MAPE: 3.741873800754547\n",
            "13 MAPE: 3.543316572904587\n",
            "14 MAPE: 2.501499094069004\n",
            "15 MAPE: 2.3696277290582657\n",
            "16 MAPE: 2.086205966770649\n",
            "17 MAPE: 2.4066153913736343\n",
            "18 MAPE: 2.806561440229416\n",
            "19 MAPE: 2.641174755990505\n",
            "Training done...\n",
            "\n",
            "\n",
            "\n",
            "Saving\n",
            "Ok\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "Training the model with Identity ...\n",
            "0 MAPE: 12.857034802436829\n",
            "1 MAPE: 3.335171937942505\n",
            "2 MAPE: 4.108953848481178\n",
            "3 MAPE: 3.224906697869301\n",
            "4 MAPE: 2.742229215800762\n",
            "5 MAPE: 2.9054509475827217\n",
            "6 MAPE: 3.0664630234241486\n",
            "7 MAPE: 3.181123360991478\n",
            "8 MAPE: 3.0555566772818565\n",
            "9 MAPE: 2.9946621507406235\n",
            "10 MAPE: 3.1999416649341583\n",
            "11 MAPE: 2.988385409116745\n",
            "12 MAPE: 2.8020424768328667\n",
            "13 MAPE: 2.5948869064450264\n",
            "14 MAPE: 2.184516564011574\n",
            "15 MAPE: 2.7089817449450493\n",
            "16 MAPE: 2.556125447154045\n",
            "17 MAPE: 2.047586254775524\n",
            "18 MAPE: 1.6990173608064651\n",
            "19 MAPE: 2.847597375512123\n",
            "Training done...\n",
            "\n",
            "\n",
            "\n",
            "Saving\n",
            "Ok\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "Training the model with ZI ...\n",
            "0 MAPE: 1.1069604195654392\n",
            "1 MAPE: 0.9488698095083237\n",
            "2 MAPE: 0.9512259624898434\n",
            "3 MAPE: 0.9600501507520676\n",
            "4 MAPE: 0.9599339216947556\n",
            "5 MAPE: 0.9496682323515415\n",
            "6 MAPE: 0.9373687207698822\n",
            "7 MAPE: 0.9264672175049782\n",
            "8 MAPE: 0.9177067317068577\n",
            "9 MAPE: 0.9108457714319229\n",
            "10 MAPE: 0.9055846370756626\n",
            "11 MAPE: 0.9014849551022053\n",
            "12 MAPE: 0.898404885083437\n",
            "13 MAPE: 0.8961357176303864\n",
            "14 MAPE: 0.89448606595397\n",
            "15 MAPE: 0.8933128789067268\n",
            "16 MAPE: 0.8924392983317375\n",
            "17 MAPE: 0.8917946368455887\n",
            "18 MAPE: 0.8913733065128326\n",
            "19 MAPE: 0.8910869248211384\n",
            "Training done...\n",
            "\n",
            "\n",
            "\n",
            "Saving\n",
            "Ok\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "Training the model with Xa_uniform ...\n",
            "0 MAPE: 9.12034660577774\n",
            "1 MAPE: 5.308424308896065\n",
            "2 MAPE: 4.049421474337578\n",
            "3 MAPE: 4.270447790622711\n",
            "4 MAPE: 4.49332632124424\n",
            "5 MAPE: 4.499350488185883\n",
            "6 MAPE: 3.5744909197092056\n",
            "7 MAPE: 3.0914468690752983\n",
            "8 MAPE: 2.7725711464881897\n",
            "9 MAPE: 2.7851052582263947\n",
            "10 MAPE: 2.6488522067666054\n",
            "11 MAPE: 2.5589069351553917\n",
            "12 MAPE: 2.7094708755612373\n",
            "13 MAPE: 3.913526237010956\n",
            "14 MAPE: 2.288888953626156\n",
            "15 MAPE: 2.6385165750980377\n",
            "16 MAPE: 3.2945413142442703\n",
            "17 MAPE: 2.7880141511559486\n",
            "18 MAPE: 2.4487104266881943\n",
            "19 MAPE: 2.2633621469140053\n",
            "Training done...\n",
            "\n",
            "\n",
            "\n",
            "Saving\n",
            "Ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ocbN2TTEVYNR",
        "colab_type": "code",
        "outputId": "29b6722a-e1b1-4127-e762-fbfd2fd7956e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "!zip -r checkpoints_rnn.zip data/cpt/\n",
        "!cp checkpoints_rnn.zip \"./drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: data/cpt/ (stored 0%)\n",
            "updating: data/cpt/rnn/ (stored 0%)\n",
            "updating: data/cpt/rnn/Xa_norm/ (stored 0%)\n",
            "updating: data/cpt/rnn/Xa_norm/checkpoint (deflated 40%)\n",
            "updating: data/cpt/rnn/Xa_norm/elf_rnn.ckpt.index (deflated 37%)\n",
            "updating: data/cpt/rnn/Xa_norm/elf_rnn.ckpt.meta (deflated 87%)\n",
            "updating: data/cpt/rnn/Xa_norm/elf_rnn.ckpt.data-00000-of-00001 (deflated 8%)\n",
            "updating: data/cpt/rnn/Xa_uniform/ (stored 0%)\n",
            "updating: data/cpt/rnn/Xa_uniform/checkpoint (deflated 40%)\n",
            "updating: data/cpt/rnn/Xa_uniform/elf_rnn.ckpt.index (deflated 37%)\n",
            "updating: data/cpt/rnn/Xa_uniform/elf_rnn.ckpt.meta (deflated 87%)\n",
            "updating: data/cpt/rnn/Xa_uniform/elf_rnn.ckpt.data-00000-of-00001 (deflated 8%)\n",
            "updating: data/cpt/rnn/ZI/ (stored 0%)\n",
            "updating: data/cpt/rnn/ZI/checkpoint (deflated 40%)\n",
            "updating: data/cpt/rnn/ZI/elf_rnn.ckpt.index (deflated 37%)\n",
            "updating: data/cpt/rnn/ZI/elf_rnn.ckpt.meta (deflated 87%)\n",
            "updating: data/cpt/rnn/ZI/elf_rnn.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "updating: data/cpt/rnn/Identity/ (stored 0%)\n",
            "updating: data/cpt/rnn/Identity/checkpoint (deflated 40%)\n",
            "updating: data/cpt/rnn/Identity/elf_rnn.ckpt.index (deflated 37%)\n",
            "updating: data/cpt/rnn/Identity/elf_rnn.ckpt.meta (deflated 87%)\n",
            "updating: data/cpt/rnn/Identity/elf_rnn.ckpt.data-00000-of-00001 (deflated 9%)\n",
            "updating: data/cpt/rnn/He/ (stored 0%)\n",
            "updating: data/cpt/rnn/He/checkpoint (deflated 40%)\n",
            "updating: data/cpt/rnn/He/elf_rnn.ckpt.index (deflated 37%)\n",
            "updating: data/cpt/rnn/He/elf_rnn.ckpt.meta (deflated 87%)\n",
            "updating: data/cpt/rnn/He/elf_rnn.ckpt.data-00000-of-00001 (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_d2kXWt1J7Uu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!unzip \"./drive/My Drive/checkpoints_rnn.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFQRE1RGh-Jp",
        "colab_type": "code",
        "outputId": "9d05e9b0-24ab-4779-9331-859ee5997461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/a/27758326/4582711\n",
        "def mean(data):\n",
        "    \"\"\"Return the sample arithmetic mean of data.\"\"\"\n",
        "    n = len(data)\n",
        "    if n < 1:\n",
        "        raise ValueError('mean requires at least one data point')\n",
        "    return sum(data)/n # in Python 2 use sum(data)/float(n)\n",
        "\n",
        "def _ss(data):\n",
        "    \"\"\"Return sum of square deviations of sequence data.\"\"\"\n",
        "    c = mean(data)\n",
        "    ss = sum((x-c)**2 for x in data)\n",
        "    return ss\n",
        "\n",
        "def stddev(data, ddof=0):\n",
        "    \"\"\"Calculates the population standard deviation\n",
        "    by default; specify ddof=1 to compute the sample\n",
        "    standard deviation.\"\"\"\n",
        "    n = len(data)\n",
        "    if n < 2:\n",
        "        raise ValueError('variance requires at least two data points')\n",
        "    ss = _ss(data)\n",
        "    pvar = ss/(n-ddof)\n",
        "    return pvar**0.5\n",
        "  \n",
        "hparams = build_hparams()\n",
        "inits = {\n",
        "         'Xa_norm': tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=\"FAN_AVG\",uniform=True), \n",
        "         'He': tf.keras.initializers.he_normal(),\n",
        "         'Identity': tf.keras.initializers.Identity(),\n",
        "         'ZI': tf.zeros_initializer(),\n",
        "         'Xa_uniform': None,  # https://github.com/tensorflow/tensorflow/blob/2a0e004358f13d6ebe936ceab1b5e7d147606583/tensorflow/python/ops/init_ops.py#L1134\n",
        "        }\n",
        "\n",
        "# Restoring\n",
        "mape_lst = []\n",
        "for init_name, init in inits.items():\n",
        "  tf.reset_default_graph()\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    inp = VarFeeder.read_vars(\"data/vars\")\n",
        "    pipe_test = InputPipe(inp, mode=ModelMode.PREDICT, batch_size=1, \n",
        "                     n_epoch=1, verbose=False, \n",
        "                     train_window=hparams.train_window, predict_window=hparams.predict_window,\n",
        "                     rand_seed=hparams.seed)\n",
        "    _, m_loss, _ = Model(inp, pipe_test, ModelMode.PREDICT, hparams, init)\n",
        "    saver = tf.train.Saver(name='eval_saver')\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    pipe_test.load_vars(sess)\n",
        "    pipe_test.init_iterator(sess)\n",
        "    \n",
        "    # Restore saved values\n",
        "    print('\\nRestoring...')\n",
        "    saver_file = f\"data/cpt/rnn/\" + init_name + \"/elf_rnn.ckpt\"\n",
        "    saver.restore(sess, saver_file)\n",
        "    print('Ok')\n",
        "\n",
        "    print(\"Testing the model\")\n",
        "    while True:\n",
        "        try:\n",
        "          error = sess.run(m_loss)\n",
        "          mape_lst.append(error)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "          break\n",
        "    print(\"Average MAPE:\", mean(mape_lst)*100)\n",
        "    print(\"Population STD:\", stddev(mape_lst))\n",
        "    print(\"Sample STD:\", stddev(mape_lst, ddof=1))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "\n",
            "Restoring...\n",
            "INFO:tensorflow:Restoring parameters from data/cpt/rnn/Xa_norm/elf_rnn.ckpt\n",
            "Ok\n",
            "Testing the model\n",
            "Average MAPE: 9.066814136341078\n",
            "Population STD: 0.06352198902115945\n",
            "Sample STD: 0.06381539468881726\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "\n",
            "Restoring...\n",
            "INFO:tensorflow:Restoring parameters from data/cpt/rnn/He/elf_rnn.ckpt\n",
            "Ok\n",
            "Testing the model\n",
            "Average MAPE: 9.269909804191338\n",
            "Population STD: 0.06577831760881489\n",
            "Sample STD: 0.06592970634210361\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "\n",
            "Restoring...\n",
            "INFO:tensorflow:Restoring parameters from data/cpt/rnn/Identity/elf_rnn.ckpt\n",
            "Ok\n",
            "Testing the model\n",
            "Average MAPE: 7.173489480185399\n",
            "Population STD: 0.061405284009034944\n",
            "Sample STD: 0.06149939177920822\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "\n",
            "Restoring...\n",
            "INFO:tensorflow:Restoring parameters from data/cpt/rnn/ZI/elf_rnn.ckpt\n",
            "Ok\n",
            "Testing the model\n",
            "Average MAPE: 5.805788629201301\n",
            "Population STD: 0.058463692786503076\n",
            "Sample STD: 0.05853085385722508\n",
            "\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "\n",
            "Restoring...\n",
            "INFO:tensorflow:Restoring parameters from data/cpt/rnn/Xa_uniform/elf_rnn.ckpt\n",
            "Ok\n",
            "Testing the model\n",
            "Average MAPE: 6.696304773245383\n",
            "Population STD: 0.06588257043029024\n",
            "Sample STD: 0.0659430964609059\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}