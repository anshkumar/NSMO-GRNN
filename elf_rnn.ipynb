{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elf_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshkumar/elf/blob/master/elf_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "74JLaETpMp5A",
        "colab_type": "code",
        "outputId": "0b28d3f1-0da6-419e-edb5-1fcf3f074d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L4sTGwL8zyEG",
        "colab_type": "code",
        "outputId": "5fcaac23-50de-4bf9-8fc1-d339d81545c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-kmVZqCe6faX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp \"./drive/My Drive/data.zip\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bxu8d4Pe8xa2",
        "colab_type": "code",
        "outputId": "f4991d64-1dcb-43e4-f402-d46d858d5d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip ./data.zip"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./data.zip\n",
            "replace data/VIC/PRICE_AND_DEMAND_201501_VIC1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AxMVqA62NvXM",
        "colab_type": "code",
        "outputId": "d09a9cca-2ca6-4f38-c641-ee5f764d9453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.7)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.2 GB  | Proc size: 1.3 GB\n",
            "GPU RAM Free: 11172MB | Used: 269MB | Util   2% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MdLtdR8KMHMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 19:05:59 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from collections import UserList, UserDict\n",
        "from typing import Union, Iterable, Tuple, Dict, Any\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os.path\n",
        "\n",
        "def _meta_file(path):\n",
        "    return os.path.join(path, 'feeder_meta.pkl')\n",
        "\n",
        "\n",
        "class VarFeeder:\n",
        "    \"\"\"\n",
        "    Builds temporary TF graph, injects variables into, and saves variables to TF checkpoint.\n",
        "    In a train time, variables can be built by build_vars() and content restored by FeederVars.restore()\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str,\n",
        "                 tensor_vars: Dict[str, Union[pd.DataFrame, pd.Series, np.ndarray]] = None,\n",
        "                 plain_vars: Dict[str, Any] = None):\n",
        "        \"\"\"\n",
        "        :param path: dir to store data\n",
        "        :param tensor_vars: Variables to save as Tensors (pandas DataFrames/Series or numpy arrays)\n",
        "        :param plain_vars: Variables to save as Python objects\n",
        "        \"\"\"\n",
        "        tensor_vars = tensor_vars or dict()\n",
        "\n",
        "        def get_values(v):\n",
        "            v = v.values if hasattr(v, 'values') else v\n",
        "            if not isinstance(v, np.ndarray):\n",
        "                v = np.array(v)\n",
        "            if v.dtype == np.float64:\n",
        "                v = v.astype(np.float32)\n",
        "            return v\n",
        "\n",
        "        values = [get_values(var) for var in tensor_vars.values()]\n",
        "\n",
        "        self.shapes = [var.shape for var in values]\n",
        "        self.dtypes = [v.dtype for v in values]\n",
        "        self.names = list(tensor_vars.keys())\n",
        "        self.path = path\n",
        "        self.plain_vars = plain_vars\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "        with open(_meta_file(path), mode='wb') as file:\n",
        "            pickle.dump(self, file)\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            tensor_vars = self._build_vars()\n",
        "            placeholders = [tf.placeholder(tf.as_dtype(dtype), shape=shape) for dtype, shape in\n",
        "                            zip(self.dtypes, self.shapes)]\n",
        "            assigners = [tensor_var.assign(placeholder) for tensor_var, placeholder in\n",
        "                         zip(tensor_vars, placeholders)]\n",
        "            feed = {ph: v for ph, v in zip(placeholders, values)}\n",
        "            saver = tf.train.Saver(self._var_dict(tensor_vars), max_to_keep=1)\n",
        "            init = tf.global_variables_initializer()\n",
        "\n",
        "            with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\n",
        "                sess.run(init)\n",
        "                sess.run(assigners, feed_dict=feed)\n",
        "                save_path = os.path.join(path, 'feeder.cpt')\n",
        "                saver.save(sess, save_path, write_meta_graph=False, write_state=False)\n",
        "\n",
        "    def _var_dict(self, variables):\n",
        "        return {name: var for name, var in zip(self.names, variables)}\n",
        "\n",
        "    def _build_vars(self):\n",
        "        def make_tensor(shape, dtype, name):\n",
        "            tf_type = tf.as_dtype(dtype)\n",
        "            if tf_type == tf.string:\n",
        "                empty = ''\n",
        "            elif tf_type == tf.bool:\n",
        "                empty = False\n",
        "            else:\n",
        "                empty = 0\n",
        "            init = tf.constant(empty, shape=shape, dtype=tf_type)\n",
        "            return tf.get_local_variable(name=name, initializer=init, dtype=tf_type)\n",
        "\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            with tf.name_scope('feeder_vars'):\n",
        "                return [make_tensor(shape, dtype, name) for shape, dtype, name in\n",
        "                        zip(self.shapes, self.dtypes, self.names)]\n",
        "\n",
        "    def create_vars(self):\n",
        "        \"\"\"\n",
        "        Builds variable list to use in current graph. Should be called during graph building stage\n",
        "        :return: variable list with additional restore and create_saver methods\n",
        "        \"\"\"\n",
        "        return FeederVars(self._var_dict(self._build_vars()), self.plain_vars, self.path)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_vars(path):\n",
        "        with open(_meta_file(path), mode='rb') as file:\n",
        "            feeder = pickle.load(file)\n",
        "        assert feeder.path == path\n",
        "        return feeder.create_vars()\n",
        "\n",
        "\n",
        "class FeederVars(UserDict):\n",
        "    def __init__(self, tensors: dict, plain_vars: dict, path):\n",
        "        variables = dict(tensors)\n",
        "        if plain_vars:\n",
        "            variables.update(plain_vars)\n",
        "        super().__init__(variables)\n",
        "        self.path = path\n",
        "        self.saver = tf.train.Saver(tensors, name='varfeeder_saver')\n",
        "        for var in variables:\n",
        "            if var not in self.__dict__:\n",
        "                self.__dict__[var] = variables[var]\n",
        "\n",
        "    def restore(self, session):\n",
        "        \"\"\"\n",
        "        Restores variable content\n",
        "        :param session: current session\n",
        "        :return: variable list\n",
        "        \"\"\"\n",
        "        self.saver.restore(session, os.path.join(self.path, 'feeder.cpt'))\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Vw4L1iCMWAh",
        "colab_type": "code",
        "outputId": "4111e63d-7db5-42f8-ba7b-6af2d04f68ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Oct  4 12:02:32 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def read_all() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads source data for training/prediction\n",
        "    \"\"\"\n",
        "    # Path to cached data\n",
        "    path = os.path.join('data', 'all.pkl')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_pickle(path)\n",
        "        return df\n",
        "    else:\n",
        "        state = {0: 'NSW', 1: 'QLD', 2: 'SA', 3: 'TAS', 4: 'VIC'}\n",
        "        year = {0: '2015', 1: '2016', 2: '2017'}\n",
        "        \n",
        "        df_nsw = pd.DataFrame()\n",
        "        df_qld = pd.DataFrame()\n",
        "        df_sa = pd.DataFrame()\n",
        "        df_tas = pd.DataFrame()\n",
        "        df_vic = pd.DataFrame()\n",
        "        \n",
        "        df_all = pd.DataFrame()\n",
        "        \n",
        "        df = {'NSW': df_nsw, 'QLD': df_qld, 'SA': df_sa, 'TAS': df_tas, 'VIC': df_vic}\n",
        "        \n",
        "        for st in state.values():\n",
        "            for ye in year.values():\n",
        "                for mn in range(1,13):\n",
        "                    if mn < 10:            \n",
        "                        dataset = pd.read_csv('./data/' + st + '/PRICE_AND_DEMAND_' + ye + '0' + str(mn) +'_' + st + '1.csv')\n",
        "                    else:\n",
        "                        dataset = pd.read_csv('./data/' + st + '/PRICE_AND_DEMAND_' + ye + str(mn) +'_' + st + '1.csv')\n",
        "                    df[st] = df[st].append(dataset.iloc[:,:3])\n",
        "            df_all = df_all.append(df[st].iloc[:,:])\n",
        "        df_all = df_all.set_index(['REGION', 'SETTLEMENTDATE'])\n",
        "        df_all.to_pickle(path)\n",
        "        return df_all\n",
        "    \n",
        "def read_x(state, start, end) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Gets source data from start to end date. Any date can be None\n",
        "    \"\"\"\n",
        "    def read_state(state):\n",
        "        df = read_all()\n",
        "        return df.loc[state]\n",
        "    df = read_state(state)\n",
        "    if start and end:\n",
        "        return df.loc[start:end]\n",
        "    elif end:\n",
        "        return df.loc[:end]\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def single_autocorr(series, lag):\n",
        "    \"\"\"\n",
        "    Autocorrelation for single data series\n",
        "    :param series: traffic series\n",
        "    :param lag: lag, days\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    s1 = series[lag:]\n",
        "    s2 = series[:-lag]\n",
        "    ms1 = np.mean(s1)\n",
        "    ms2 = np.mean(s2)\n",
        "    ds1 = s1 - ms1\n",
        "    ds2 = s2 - ms2\n",
        "    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n",
        "    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n",
        "\n",
        "def batch_autocorr(series, lag, starts, ends, backoffset=0):\n",
        "    \"\"\"\n",
        "    Calculate autocorrelation for batch (many time series at once)\n",
        "    :param data: Time series\n",
        "    :param lag: Autocorrelation lag\n",
        "    :param starts: Start index for series\n",
        "    :param ends: End index for series\n",
        "    :param backoffset: Offset from the series end, days.\n",
        "    :return: autocorrelation, shape [n_series].\n",
        "    \"\"\"\n",
        "    n_series = series.shape[0]\n",
        "    max_end = n_series - backoffset\n",
        "    \n",
        "    end = min(ends, max_end)\n",
        "    series = series[starts:end]\n",
        "    c_365 = single_autocorr(series, lag)\n",
        "    c_364 = single_autocorr(series, lag-1)\n",
        "    c_366 = single_autocorr(series, lag+1)\n",
        "    # Average value between exact lag and two nearest neighborhs for smoothness\n",
        "    corr = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n",
        "    return corr #, support\n",
        "\n",
        "def prepare_data(start, end, state) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(series)\n",
        "    :param start: start date of effective time interval, can be None to start from beginning\n",
        "    :param end: end date of effective time interval, can be None to return all data\n",
        "    :param state: state of series\n",
        "    :return: log1p(series)\n",
        "    \"\"\"\n",
        "    df = read_x(state, start, end)\n",
        "    return np.log1p(df.fillna(0))\n",
        "\n",
        "def normalize(values: np.ndarray):\n",
        "    return (values - values.mean()) / np.std(values)\n",
        "\n",
        "def lag_indexes(begin, end) -> List[pd.Series]:\n",
        "    \"\"\"\n",
        "    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n",
        "    :param begin: start of date range\n",
        "    :param end: end of date range\n",
        "    :return: List of 4 Series, one for each lag. For each Series, index is date in range(begin, end), value is an index\n",
        "     of target (lagged) date in a same Series. If target date is out of (begin,end) range, index is -1\n",
        "    \"\"\"\n",
        "    dr = pd.date_range(begin, end, freq='0.5H')\n",
        "    # key is date, value is day index\n",
        "    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n",
        "\n",
        "    def lag(offset):\n",
        "        dates = dr - offset\n",
        "        return pd.Series(data=base_index.loc[dates].fillna(-1).astype(np.int32).values, index=dr)\n",
        "\n",
        "    return [lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]\n",
        "\n",
        "def run():\n",
        "    \n",
        "    data_dir = \"data/vars\"\n",
        "    state = \"NSW1\"\n",
        "    add_end = 0\n",
        "    start = \"2015/01/01 00:30:00\"\n",
        "    end = \"2018/01/01 00:00:00\"\n",
        "    corr_backoffset = 0\n",
        "    \n",
        "    # Get the data\n",
        "    df = prepare_data(start, end, state)\n",
        "    \n",
        "    # Our working date range\n",
        "    data_start, data_end = pd.to_datetime(df.first_valid_index()), pd.to_datetime(df.last_valid_index())\n",
        "    \n",
        "    # We have to project some date-dependent features (day of week, etc) to the future dates for prediction\n",
        "    features_end = data_end + pd.Timedelta(add_end/2, unit='h')\n",
        "    print(f\"start: {data_start}, end:{data_end}, features_end:{features_end}\")\n",
        "    \n",
        "    starts = 0\n",
        "    ends = df.shape[0]\n",
        "    \n",
        "    # Yearly(annual) autocorrelation\n",
        "    year_autocorr = batch_autocorr(df.values, 365, starts, ends, corr_backoffset)\n",
        "    \n",
        "    # Quarterly autocorrelation\n",
        "    quarter_autocorr = batch_autocorr(df.values, int(round(365.25/4)), starts, ends, corr_backoffset)\n",
        "    \n",
        "    # Make time-dependent features\n",
        "    features = pd.date_range(data_start, features_end, freq='0.5H')\n",
        "    #dow = normalize(features_days.dayofweek.values)\n",
        "    week_period = 7 / (2 * np.pi)\n",
        "    dow_norm = features.dayofweek.values / week_period\n",
        "    dow = np.stack([np.cos(dow_norm), np.sin(dow_norm)], axis=-1)\n",
        "    \n",
        "     # Assemble indices for quarterly lagged data\n",
        "    lagged_ix = np.stack(lag_indexes(data_start, features_end), axis=-1)\n",
        "    \n",
        "    # Assemble final output\n",
        "    tensors = dict(\n",
        "        hits=df,\n",
        "        lagged_ix=lagged_ix,\n",
        "        page_ix=df.index.values,\n",
        "        dow=dow\n",
        "        )\n",
        "    plain = dict(\n",
        "        features=len(features),\n",
        "        year_autocorr=year_autocorr,\n",
        "        quarter_autocorr=quarter_autocorr,\n",
        "        data_size=df.shape[0],\n",
        "        data_start=data_start,\n",
        "        data_end=data_end,\n",
        "        features_end=features_end\n",
        "        )\n",
        "\n",
        "    # Store data to the disk\n",
        "    VarFeeder(data_dir, tensors, plain)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    python make_features.py data/vars --add_end=0 --start=\"2015/01/01 00:30:00\" --end=\"2018/01/01 00:00:00\"\n",
        "    \"\"\"\n",
        "    run()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start: 2015-01-01 00:30:00, end:2018-01-01 00:00:00, features_end:2018-01-01 00:00:00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: FutureWarning: \n",
            "Passing list-likes to .loc or [] with any missing label will raise\n",
            "KeyError in the future, you can use .reindex() as an alternative.\n",
            "\n",
            "See the documentation here:\n",
            "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zJABrGwVhIPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 20:09:16 2018\n",
        "\n",
        "@author: vedanshu\n",
        "\"\"\"\n",
        "\n",
        "from enum import Enum\n",
        "import tensorflow as tf\n",
        "\n",
        "class ModelMode(Enum):\n",
        "  TRAIN = 0,\n",
        "  EVAL = 1,\n",
        "  PREDICT = 2\n",
        "\n",
        "class InputPipe:\n",
        "  def __init__(self, inp: VarFeeder, mode: ModelMode, n_epoch=None,\n",
        "               batch_size=32, runs_in_burst=1, verbose=True, predict_window=48, train_window=36, back_offset=0,\n",
        "               train_skip_first=0, rand_seed=None):\n",
        "      \"\"\"\n",
        "      Create data preprocessing pipeline\n",
        "      :param inp: Raw input data\n",
        "      :param features: Features tensors (subset of data in inp)\n",
        "      :param mode: Train/Predict/Eval mode selector\n",
        "      :param n_epoch: Number of epochs. Generates endless data stream if None\n",
        "      :param batch_size: (https://www.tensorflow.org/guide/datasets#batching_dataset_elements)\n",
        "      :param n_splits: number of time series split\n",
        "      :param runs_in_burst: How many batches can be consumed at short time interval (burst). Multiplicator for prefetch() (https://www.tensorflow.org/performance/datasets_performance)\n",
        "      :param verbose: Print additional information during graph construction\n",
        "      :param predict_window: Number of days to predict\n",
        "      :param train_window: Use train_window days for traning\n",
        "      :param back_offset: Don't use back_offset days at the end of timeseries\n",
        "      :param train_skip_first: Don't use train_skip_first days at the beginning of timeseries\n",
        "      :param rand_seed:\n",
        "      \"\"\"\n",
        "      self.inp = inp\n",
        "      self.batch_size = batch_size\n",
        "      self.rand_seed = rand_seed\n",
        "      self.back_offset = back_offset\n",
        "      self.start_offset = train_skip_first\n",
        "      self.train_window = train_window\n",
        "      self.predict_window = predict_window\n",
        "      self.mode = mode\n",
        "      self.verbose = verbose\n",
        "      \n",
        "      list_half_hourly_load = self.inp.hits[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      list_half_hourly_load = list_half_hourly_load / tf.linalg.norm(list_half_hourly_load)\n",
        "      matrix_hits = list_half_hourly_load[:-(list_half_hourly_load.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_hits = tf.reshape(matrix_hits, [-1, (train_window + predict_window)])\n",
        "      \n",
        "      matrix_dow = self.inp.dow[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      matrix_dow = matrix_dow[:-(matrix_dow.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_dow = tf.reshape(matrix_dow, [-1, (train_window + predict_window), 2])\n",
        "      \n",
        "      # Convert -1 to 0 for gather(), it don't accept anything exotic\n",
        "      lags_indices = tf.maximum(self.inp.lagged_ix, 0)\n",
        "      # Translate lag indexes to hit values\n",
        "      lagged_hit = tf.gather(list_half_hourly_load, lags_indices)\n",
        "      lagged_hit = lagged_hit / tf.linalg.norm(lagged_hit)\n",
        "      lagged_hit = lagged_hit[train_skip_first:(-back_offset if back_offset > 0 else None)]\n",
        "      matrix_lagged_hit = lagged_hit[:-(lagged_hit.shape[0].value % (train_window + predict_window))]\n",
        "      matrix_lagged_hit = tf.reshape(matrix_lagged_hit, [-1, (train_window + predict_window), 4])\n",
        "      \n",
        "      x_hits = matrix_hits[:, :train_window]\n",
        "      y_hits = matrix_hits[:, train_window:]\n",
        "      \n",
        "      x_dow = matrix_dow[:, :train_window]\n",
        "      y_dow = matrix_dow[:, train_window:]\n",
        "      \n",
        "      x_lagged_hit = matrix_lagged_hit[:, :train_window]\n",
        "      y_lagged_hit = matrix_lagged_hit[:, train_window:]\n",
        "\n",
        "      def tileFeatures(window):\n",
        "        # [n_features] => [window, n_features]\n",
        "        flat_features = tf.stack([self.inp.year_autocorr, self.inp.quarter_autocorr], axis=0 )\n",
        "        features = tf.expand_dims(flat_features, axis=0)\n",
        "        features = tf.tile(features, [window,1])\n",
        "\n",
        "        # [train_window, n_features] => [size, window, n_features]\n",
        "        features = tf.expand_dims(features, axis=0)\n",
        "        features = tf.tile(features, [x_hits.shape[0].value, 1,1])\n",
        "        return features\n",
        "      \n",
        "      x_features = tf.concat((tf.reshape(x_hits, [x_hits.shape[0].value,train_window,1]),\n",
        "                                   x_dow, x_lagged_hit, \n",
        "                                   tileFeatures(train_window) \n",
        "                                   ), axis = 2)\n",
        "      \n",
        "      y_features = tf.concat((y_dow, \n",
        "                              y_lagged_hit,\n",
        "                              tileFeatures(predict_window)\n",
        "                             ), axis = 2)\n",
        "      \n",
        "      y_labels = tf.reshape(y_hits, [y_hits.shape[0].value, predict_window, 1])\n",
        "      \n",
        "      # Assume that each row of `features` corresponds to the same row as `labels`.\n",
        "      assert x_features.shape[0] == y_labels.shape[0].value\n",
        "      \n",
        "      dataset = tf.data.Dataset.from_tensor_slices((x_features, y_features, y_labels))\n",
        "      # [Other transformations on `dataset`...]\n",
        "      #dataset = dataset.shuffle(seed=rand_seed)\n",
        "      dataset = dataset.repeat(n_epoch)  # Repeat the input indefinitely.\n",
        "      dataset = dataset.batch(batch_size)\n",
        "      dataset = dataset.prefetch(runs_in_burst * 2)\n",
        "\n",
        "      self.iterator = dataset.make_initializable_iterator()\n",
        "      self.x_feature, self.y_feature, self.y_true = self.iterator.get_next()\n",
        "      self.encoder_features_depth = self.x_feature.shape[2].value\n",
        "\n",
        "  def load_vars(self, session):\n",
        "      self.inp.restore(session)\n",
        "\n",
        "  def init_iterator(self, session):\n",
        "      session.run(self.iterator.initializer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xleAskOyLDSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.training as training\n",
        "\n",
        "params = dict(\n",
        "    epochs = 2000,\n",
        "    batch_size=256,\n",
        "    train_window=24,\n",
        "    predict_window=48,\n",
        "    train_skip_first=0,\n",
        "    seed = 1,\n",
        "    rnn_depth=138,\n",
        "    encoder_readout_dropout=0.6415488109353416,\n",
        "\n",
        "    encoder_rnn_layers=1,\n",
        "    decoder_rnn_layers=2,\n",
        "\n",
        "    decoder_input_dropout=[1.0, 1.0, 1.0],\n",
        "    decoder_output_dropout=[0.925, 0.925, 1.0],\n",
        "    decoder_state_dropout=[0.98, 0.98, 0.995],\n",
        "    decoder_variational_dropout=[False, False, False],\n",
        "    decoder_candidate_l2=0.0,\n",
        "    decoder_gates_l2=0.0,\n",
        "    gate_dropout=0.9275441207192259,\n",
        "    gate_activation='none',\n",
        "    encoder_dropout=0.0,\n",
        "    encoder_stability_loss=0.0,\n",
        "    encoder_activation_loss=1e-06,\n",
        "    decoder_stability_loss=0.0,\n",
        "    decoder_activation_loss=1e-06,\n",
        ")\n",
        "\n",
        "def build_hparams():\n",
        "  return training.HParams(**params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "24LhzDYat2bK",
        "colab_type": "code",
        "outputId": "6087a15c-d545-4df9-ff5e-e866588b0274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1262
        }
      },
      "cell_type": "code",
      "source": [
        "def mape(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + 1e-10))))\n",
        "\n",
        "hparams = build_hparams()\n",
        "tf.reset_default_graph()\n",
        "inp = VarFeeder.read_vars(\"data/vars\")\n",
        "pipe_train = InputPipe(inp, mode=ModelMode.TRAIN, batch_size=hparams.batch_size, \n",
        "                       n_epoch=hparams.epochs, verbose=False, \n",
        "                       train_window=hparams.train_window, predict_window=hparams.predict_window,\n",
        "                       rand_seed=hparams.seed)\n",
        "\n",
        "def build_rnn():\n",
        "  return tf.contrib.cudnn_rnn.CudnnGRU(num_layers=hparams.encoder_rnn_layers, num_units=hparams.rnn_depth,\n",
        "             direction='unidirectional',\n",
        "             dropout=hparams.encoder_dropout, seed=hparams.seed)\n",
        "\n",
        "cuda_model = build_rnn()\n",
        "\n",
        "rnn_time_input = pipe_train.x_feature\n",
        "\n",
        "rnn_out, (rnn_state,) = cuda_model(inputs=rnn_time_input)\n",
        "\n",
        "learning_rate = 0.001   #small learning rate so we don't overshoot the minimum\n",
        "\n",
        "fc_input = tf.reshape(rnn_out, [-1, hparams.train_window*hparams.rnn_depth])\n",
        "fc_outputs = tf.layers.dense(fc_input, hparams.predict_window, \n",
        "                             kernel_initializer=tf.zeros_initializer(),\n",
        "                            activation = tf.tanh)       #specify the type of layer (dense)\n",
        "outputs = tf.reshape(fc_outputs, [-1, hparams.predict_window, 1])          #shape of results\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - pipe_train.y_true))    #define the cost function which evaluates the quality of our model\n",
        "mape_loss = mape(pipe_train.y_true, outputs)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)          #gradient descent method\n",
        "training_op = optimizer.minimize(loss)          #train the result of the application of the cost_function                                 \n",
        "\n",
        "model_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "# Make EMA object and update interal variables after optimization step\n",
        "ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
        "with tf.control_dependencies([training_op]):\n",
        "    train_op = ema.apply(model_vars)\n",
        "\n",
        "# Transfer EMA values to original variables\n",
        "retrieve_ema_weights_op = tf.group([tf.assign(var, ema.average(var)) for var in model_vars])\n",
        "\n",
        "init = tf.global_variables_initializer()           #initialize all the variables\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    init.run()\n",
        "    inp.restore(sess)\n",
        "    pipe_train.init_iterator(sess)\n",
        "\n",
        "    for ep in range(hparams.epochs):\n",
        "        _, error = sess.run([training_op, mape_loss])\n",
        "        if ep > 1000:\n",
        "          # Copy EMA values to weights\n",
        "          sess.run(retrieve_ema_weights_op)\n",
        "        if ep % 100 == 0:\n",
        "            print(ep, \"MAPE:\", error*100)\n",
        "print(\"\\n\")\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from data/vars/feeder.cpt\n",
            "0 MAPE: 100.0\n",
            "100 MAPE: 13.244129717350006\n",
            "200 MAPE: 3.5378877073526382\n",
            "300 MAPE: 2.057003602385521\n",
            "400 MAPE: 1.6309591010212898\n",
            "500 MAPE: 2.246139384806156\n",
            "600 MAPE: 2.732541970908642\n",
            "700 MAPE: 1.8203072249889374\n",
            "800 MAPE: 2.3227766156196594\n",
            "900 MAPE: 1.9006291404366493\n",
            "1000 MAPE: 1.7891338095068932\n",
            "1100 MAPE: 100.0\n",
            "1200 MAPE: 100.0\n",
            "1300 MAPE: 100.0\n",
            "1400 MAPE: 100.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-993ca263d899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmape_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0;31m# Copy EMA values to weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}